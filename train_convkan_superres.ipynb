{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# ConvKAN-Based Image Super-Resolution Training\n",
    "\n",
    "This notebook implements a Convolutional Kolmogorov-Arnold Network (ConvKAN) architecture for image super-resolution. ConvKAN replaces traditional convolutional layers with learnable activation functions based on splines, potentially offering better feature learning capabilities.\n",
    "\n",
    "## Project Overview\n",
    "- **Task**: Image Super-Resolution - enhance bicubic-upsampled images\n",
    "- **Architecture**: ConvKAN with residual blocks\n",
    "- **Input**: Low-resolution images (64×64) upsampled to 256×256 via bicubic interpolation\n",
    "- **Output**: High-resolution images at 256×256 with restored details\n",
    "- **Loss Function**: L1 Loss (Mean Absolute Error)\n",
    "\n",
    "## Key Differences from U-Net\n",
    "- Uses ConvKAN layers instead of standard Conv2d\n",
    "- Employs residual connections for deeper feature learning\n",
    "- No downsampling - operates at constant 256×256 resolution\n",
    "- Focuses on detail restoration rather than spatial transformation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports_header",
   "metadata": {},
   "source": [
    "## 1. Import Dependencies and Environment Setup\n",
    "\n",
    "### GPU Configuration\n",
    "- `CUDA_VISIBLE_DEVICES`: Select which GPU to use (default: GPU 1)\n",
    "- `PYTORCH_CUDA_ALLOC_CONF`: Reduce memory fragmentation with expandable segments\n",
    "\n",
    "### Key Libraries\n",
    "- **convkan**: The ConvKAN layer implementation\n",
    "- **torch**: PyTorch deep learning framework\n",
    "- **PIL**: Image loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Select GPU (change to '0' for GPU0, or remove line to use default)\n",
    "os.environ.setdefault('CUDA_VISIBLE_DEVICES', '1')\n",
    "# Reduce GPU memory fragmentation\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# Import ConvKAN - install via: pip install convkan\n",
    "try:\n",
    "    from convkan import ConvKAN, LayerNorm2D\n",
    "    print(\"✓ ConvKAN imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"ERROR: convkan is not installed. Run: pip install convkan\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "device_header",
   "metadata": {},
   "source": [
    "## 2. Device Configuration\n",
    "\n",
    "Check CUDA availability and display GPU information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "device",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset_header",
   "metadata": {},
   "source": "## 3. Dataset Class\n\n### SRDataset: Super-Resolution Dataset\n\n**Key Change: Using PixelShuffle Architecture (Memory-Efficient)**\n\n**Data Flow:**\n1. Load LR image from disk (64×64)\n2. Load corresponding HR image (256×256)\n3. **Resize LR to 128×128** ← Network processes at this size (saves memory!)\n4. **Resize HR to 256×256** ← Keep original size\n5. Model uses PixelShuffle internally: 128×128 → 256×256\n\n**Why This is More Efficient:**\n- ConvKAN processes at 128×128, using only 1/4 memory vs 256×256\n- PixelShuffle upsampling is efficient (just rearranges pixels)\n- This architecture is proven to work in the reference implementation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataset",
   "metadata": {},
   "outputs": [],
   "source": "class SRDataset(Dataset):\n    \"\"\"Super-Resolution Dataset with efficient resizing for PixelShuffle\"\"\"\n    def __init__(self, hr_dir, lr_dir, lr_size=128, hr_size=256, transform=None):\n        self.hr_dir = Path(hr_dir)\n        self.lr_dir = Path(lr_dir)\n        self.lr_size = lr_size  # Network processes at this size\n        self.hr_size = hr_size  # Target output size\n        self.transform = transform\n        \n        # Load all image file paths\n        self.hr_images = sorted(list(self.hr_dir.glob('*.png')))\n        self.lr_images = sorted(list(self.lr_dir.glob('*.png')))\n        \n        # Verify dataset integrity\n        if not self.hr_images or not self.lr_images:\n            raise IOError(f\"No images found in {hr_dir} or {lr_dir}\")\n        \n        assert len(self.hr_images) == len(self.lr_images), \\\n            f\"Mismatch: {len(self.hr_images)} HR vs {len(self.lr_images)} LR images\"\n        \n        print(f\"Dataset loaded: {len(self.hr_images)} image pairs\")\n        \n        # Check actual image sizes\n        sample_lr = Image.open(self.lr_images[0])\n        sample_hr = Image.open(self.hr_images[0])\n        print(f\"Original LR size: {sample_lr.size}\")\n        print(f\"Original HR size: {sample_hr.size}\")\n        print(f\"LR will be resized to: {lr_size}×{lr_size} (network input)\")\n        print(f\"HR will be resized to: {hr_size}×{hr_size} (target output)\")\n        \n    def __len__(self):\n        return len(self.hr_images)\n    \n    def __getitem__(self, idx):\n        # Load images\n        hr_img = Image.open(self.hr_images[idx]).convert('RGB')\n        lr_img = Image.open(self.lr_images[idx]).convert('RGB')\n        \n        # Resize to appropriate sizes for PixelShuffle architecture\n        lr_img = lr_img.resize((self.lr_size, self.lr_size), Image.BICUBIC)\n        hr_img = hr_img.resize((self.hr_size, self.hr_size), Image.BICUBIC)\n        \n        # Apply transforms (to tensor)\n        if self.transform:\n            hr_img = self.transform(hr_img)\n            lr_img = self.transform(lr_img)\n        \n        return lr_img, hr_img"
  },
  {
   "cell_type": "markdown",
   "id": "model_header",
   "metadata": {},
   "source": "## 4. ConvKAN Model Architecture\n\n### What is ConvKAN?\n\nConvKAN (Convolutional Kolmogorov-Arnold Network) replaces traditional convolutional layers with learnable spline-based activation functions.\n\n**Key Architecture: PixelShuffle Upsampling**\n\n```\nInput: RGB 128×128 (3 channels)\n    ↓\n[Head: ConvKAN 3→16 channels]  ← Initial feature extraction\n    ↓\n[Body: 2× ResBlock (16 ch)]    ← Feature processing at 128×128\n    ↓\n[Upsample: ConvKAN 16→64 + PixelShuffle(2×)]  ← 128×128 → 256×256\n    ↓\n[Tail: Conv2d 16→3]            ← RGB reconstruction\n    ↓\nOutput: RGB 256×256 (3 channels)\n```\n\n**Why PixelShuffle is Memory-Efficient:**\n- Most computation happens at 128×128 (small resolution)\n- PixelShuffle just rearranges pixels (no heavy computation)\n- Memory usage: 128×128 vs 256×256 = 4× reduction!\n- This matches the proven reference implementation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model",
   "metadata": {},
   "outputs": [],
   "source": "class ConvKANResBlock(nn.Module):\n    \"\"\"\n    Residual block with ConvKAN layers\n    \n    Architecture:\n        Input (C channels, H×W)\n            ↓\n        ConvKAN(C→C, 3×3) + LayerNorm\n            ↓\n        ConvKAN(C→C, 3×3) + LayerNorm\n            ↓\n        Add residual connection\n            ↓\n        Output (C channels, H×W)\n    \"\"\"\n    def __init__(self, channels):\n        super().__init__()\n        self.block = nn.Sequential(\n            ConvKAN(channels, channels, kernel_size=3, padding=1),\n            LayerNorm2D(channels),\n            ConvKAN(channels, channels, kernel_size=3, padding=1),\n            LayerNorm2D(channels)\n        )\n\n    def forward(self, x):\n        return x + self.block(x)\n\n\nclass ConvKAN_SR(nn.Module):\n    \"\"\"\n    ConvKAN Super-Resolution with PixelShuffle Upsampling\n    \n    Full Architecture Breakdown:\n    ============================\n    \n    Layer                   Input Shape         Output Shape        Memory\n    ───────────────────────────────────────────────────────────────────────\n    INPUT                   [B, 3, 128, 128]    -                   0.2 MB\n    \n    HEAD (Feature Extraction):\n    ConvKAN(3→16)           [B, 3, 128, 128]    [B, 16, 128, 128]   1 MB\n    \n    BODY (Feature Processing at 128×128):\n    ResBlock1               [B, 16, 128, 128]   [B, 16, 128, 128]   1 MB\n    ResBlock2               [B, 16, 128, 128]   [B, 16, 128, 128]   1 MB\n    + Long Residual         [B, 16, 128, 128]   [B, 16, 128, 128]   -\n    \n    UPSAMPLE (128×128 → 256×256):\n    ConvKAN(16→64)          [B, 16, 128, 128]   [B, 64, 128, 128]   4 MB\n    PixelShuffle(2×)        [B, 64, 128, 128]   [B, 16, 256, 256]   0 MB\n    LayerNorm               [B, 16, 256, 256]   [B, 16, 256, 256]   0 MB\n    \n    TAIL (RGB Reconstruction):\n    Conv2d(16→3)            [B, 16, 256, 256]   [B, 3, 256, 256]    0.01 MB\n    \n    ───────────────────────────────────────────────────────────────────────\n    Total Parameters: ~350K (0.35M)\n    Peak Memory (FP16): ~3-4 GB (batch_size=1, with gradients)\n    ───────────────────────────────────────────────────────────────────────\n    \n    Key Benefits:\n    ─────────────\n    1. **Processes at 128×128**: 4× less memory than 256×256\n    2. **PixelShuffle upsampling**: Efficient spatial expansion\n    3. **Minimal depth**: Only 2 ResBlocks for fast training\n    4. **Proven architecture**: Based on working reference code\n    \"\"\"\n    def __init__(self, in_channels=3, out_channels=3, base_filters=16, n_res_blocks=2, upscale_factor=2):\n        super().__init__()\n        \n        print(f\"\\n{'='*60}\")\n        print(f\"Initializing ConvKAN_SR with PixelShuffle\")\n        print(f\"{'='*60}\")\n        print(f\"  Input channels:    {in_channels}\")\n        print(f\"  Output channels:   {out_channels}\")\n        print(f\"  Base filters:      {base_filters}\")\n        print(f\"  Residual blocks:   {n_res_blocks}\")\n        print(f\"  Upscale factor:    {upscale_factor}×\")\n        print(f\"  Processing size:   128×128\")\n        print(f\"  Output size:       256×256\")\n        print(f\"{'='*60}\\n\")\n        \n        # Head: Initial feature extraction\n        self.head = ConvKAN(in_channels, base_filters, kernel_size=3, padding=1)\n        \n        # Body: Feature processing with residual blocks\n        body = [ConvKANResBlock(base_filters) for _ in range(n_res_blocks)]\n        self.body = nn.Sequential(*body)\n        \n        # Upsample: PixelShuffle to increase resolution\n        # ConvKAN expands channels by upscale_factor^2, then PixelShuffle rearranges\n        self.upsample = nn.Sequential(\n            ConvKAN(base_filters, base_filters * (upscale_factor ** 2), kernel_size=3, padding=1),\n            nn.PixelShuffle(upscale_factor),\n            LayerNorm2D(base_filters)\n        )\n        \n        # Tail: Map features back to RGB (standard Conv2d for efficiency)\n        self.tail = nn.Conv2d(base_filters, out_channels, kernel_size=3, padding=1)\n        \n        # Count parameters\n        total_params = sum(p.numel() for p in self.parameters())\n        print(f\"Total parameters: {total_params:,} ({total_params/1e6:.2f}M)\")\n        \n    def forward(self, x):\n        \"\"\"\n        Forward pass with PixelShuffle upsampling\n        \n        Args:\n            x: Input tensor [B, 3, 128, 128]\n        \n        Returns:\n            Output tensor [B, 3, 256, 256]\n        \"\"\"\n        # Extract initial features at 128×128\n        x = self.head(x)  # [B, 3, 128, 128] → [B, 16, 128, 128]\n        \n        # Process features with long skip connection\n        res = x\n        x = self.body(x)  # [B, 16, 128, 128] → [B, 16, 128, 128]\n        x = x + res       # Long residual connection\n        \n        # Upsample to 256×256 using PixelShuffle\n        x = self.upsample(x)  # [B, 16, 128, 128] → [B, 16, 256, 256]\n        \n        # Reconstruct RGB image\n        x = self.tail(x)  # [B, 16, 256, 256] → [B, 3, 256, 256]\n        \n        return x"
  },
  {
   "cell_type": "markdown",
   "id": "config_header",
   "metadata": {},
   "source": "## 5. Training Configuration\n\n### Hyperparameters\n\n**Memory-Optimized Settings with PixelShuffle:**\n- **BATCH_SIZE = 1**: Minimal batch size for ConvKAN\n- **BASE_FILTERS = 16**: Reduced from typical 64\n- **N_RES_BLOCKS = 2**: Minimal depth for faster training\n- **LR_SIZE = 128**: Network processes at this resolution\n- **HR_SIZE = 256**: Target output via PixelShuffle (2× upsampling)\n- **LEARNING_RATE = 1e-4**: Conservative rate\n- **NUM_EPOCHS = 50**: Sufficient for convergence\n\n**Architecture Comparison:**\n- Old (OOM): Process at 256×256 → 4× memory usage\n- New (Works): Process at 128×128 + PixelShuffle → Fits in GPU!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": "# Hyperparameters - PixelShuffle architecture (proven to work)\nBATCH_SIZE = 1\nLEARNING_RATE = 1e-4\nNUM_EPOCHS = 50\nTRAIN_SPLIT = 0.9\n\n# Model architecture - Matching reference implementation\nBASE_FILTERS = 16\nN_RES_BLOCKS = 2\nUPSCALE_FACTOR = 2  # 128×128 → 256×256\n\n# Image sizes for PixelShuffle\nLR_SIZE = 128  # Network processes at this size\nHR_SIZE = 256  # Target output size\n\n# Data paths\nHR_DIR = './dataset/high_resolution'\nLR_DIR = './dataset/low_resolution'\nCHECKPOINT_DIR = Path('./checkpoints_convkan')\nCHECKPOINT_DIR.mkdir(exist_ok=True)\n\nprint(f\"Batch size: {BATCH_SIZE}\")\nprint(f\"Learning rate: {LEARNING_RATE}\")\nprint(f\"Epochs: {NUM_EPOCHS}\")\nprint(f\"Model: base_filters={BASE_FILTERS}, n_res_blocks={N_RES_BLOCKS}\")\nprint(f\"Architecture: Process at {LR_SIZE}×{LR_SIZE}, output {HR_SIZE}×{HR_SIZE}\")\nprint(f\"✓ Using PixelShuffle for memory efficiency\")"
  },
  {
   "cell_type": "markdown",
   "id": "data_header",
   "metadata": {},
   "source": "## 6. Data Loading and Preprocessing\n\n### Data Pipeline with PixelShuffle Architecture:\n1. **Transform**: Convert PIL images to tensors (range [0, 1])\n2. **Dataset**: Resize LR to 128×128, HR to 256×256\n3. **Split**: 90% train, 10% validation\n4. **DataLoader**: \n   - `num_workers=0` to avoid multiprocessing issues in notebooks\n   - `pin_memory=True` for faster GPU transfer\n\n**Key Difference from Previous Version:**\n- LR resized to **128×128** (not 256×256!) → network processes at smaller size\n- HR remains at **256×256** → target output\n- Model uses PixelShuffle to bridge the resolution gap"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data",
   "metadata": {},
   "outputs": [],
   "source": "# Transform: to tensor only\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n])\n\n# Load dataset with PixelShuffle-appropriate sizes\nfull_dataset = SRDataset(HR_DIR, LR_DIR, lr_size=LR_SIZE, hr_size=HR_SIZE, transform=transform)\n\n# Split train/val\ntrain_size = int(TRAIN_SPLIT * len(full_dataset))\nval_size = len(full_dataset) - train_size\ntrain_dataset, val_dataset = torch.utils.data.random_split(\n    full_dataset, [train_size, val_size],\n    generator=torch.Generator().manual_seed(42)  # Reproducible split\n)\n\n# Create data loaders\ntrain_loader = DataLoader(\n    train_dataset, \n    batch_size=BATCH_SIZE, \n    shuffle=True, \n    num_workers=0,  # Avoid multiprocessing issues in notebooks\n    pin_memory=True\n)\nval_loader = DataLoader(\n    val_dataset, \n    batch_size=BATCH_SIZE, \n    shuffle=False, \n    num_workers=0,\n    pin_memory=True\n)\n\nprint(f\"\\nDataset split:\")\nprint(f\"  Training: {train_size} images ({len(train_loader)} batches)\")\nprint(f\"  Validation: {val_size} images ({len(val_loader)} batches)\")\nprint(f\"  LR input size: {LR_SIZE}×{LR_SIZE}\")\nprint(f\"  HR target size: {HR_SIZE}×{HR_SIZE}\")"
  },
  {
   "cell_type": "markdown",
   "id": "2nukh7ey865",
   "source": "### Verify Dataset Loading\n\nLet's visualize 2 random samples to verify the dataset is loaded correctly.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "x02qofusmj7",
   "source": "import matplotlib.pyplot as plt\nimport random\n\n# Select 2 random samples\nsample_indices = random.sample(range(len(full_dataset)), 2)\n\nfig, axes = plt.subplots(2, 2, figsize=(10, 10))\n\nfor i, idx in enumerate(sample_indices):\n    lr_img, hr_img = full_dataset[idx]\n    \n    # Convert to numpy for display\n    lr_np = lr_img.numpy().transpose(1, 2, 0)\n    hr_np = hr_img.numpy().transpose(1, 2, 0)\n    \n    # Display LR\n    axes[i, 0].imshow(lr_np)\n    axes[i, 0].set_title(f'Sample {i+1}: LR Input\\nShape: {lr_img.shape} (128×128)')\n    axes[i, 0].axis('off')\n    \n    # Display HR\n    axes[i, 1].imshow(hr_np)\n    axes[i, 1].set_title(f'Sample {i+1}: HR Target\\nShape: {hr_img.shape} (256×256)')\n    axes[i, 1].axis('off')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"✓ Dataset verification complete\")\nprint(f\"  LR images: Resized to 128×128 (network input)\")\nprint(f\"  HR images: Resized to 256×256 (target output)\")\nprint(f\"  Model will upsample 128×128 → 256×256 using PixelShuffle\")\nprint(f\"  Total samples: {len(full_dataset)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "init_header",
   "metadata": {},
   "source": [
    "## 7. Model Initialization\n",
    "\n",
    "### Components:\n",
    "- **Model**: ConvKAN_SR with reduced filters for memory efficiency\n",
    "- **Loss Function**: L1 Loss (MAE) - preserves sharp edges better than MSE\n",
    "- **Optimizer**: AdamW with weight decay for regularization\n",
    "- **Mixed Precision**: GradScaler for faster training with float16\n",
    "\n",
    "**Parameter Count:**\n",
    "ConvKAN has more parameters than standard Conv2d due to learnable spline coefficients. The model will display total trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "init",
   "metadata": {},
   "outputs": [],
   "source": "# Initialize model with PixelShuffle architecture\nmodel = ConvKAN_SR(\n    in_channels=3, \n    out_channels=3, \n    base_filters=BASE_FILTERS, \n    n_res_blocks=N_RES_BLOCKS,\n    upscale_factor=UPSCALE_FACTOR\n).to(device)\n\n# Loss and optimizer\ncriterion = nn.L1Loss()\noptimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n\n# Mixed precision training\nscaler = torch.amp.GradScaler('cuda')\n\n# Enable memory optimizations\nif device.type == 'cuda':\n    # Enable TF32 for faster training on Ampere+ GPUs\n    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.allow_tf32 = True\n    \n    # Enable cudnn benchmarking for faster convolutions\n    torch.backends.cudnn.benchmark = True\n    \n    # Set memory allocator settings\n    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128,expandable_segments:True'\n    \n    print(\"✓ GPU optimizations enabled:\")\n    print(\"  - TF32 precision\")\n    print(\"  - CuDNN benchmarking\")\n    print(\"  - Memory allocator optimized\")\n\n# Print model info\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"\\nModel Statistics:\")\nprint(f\"  Total parameters: {total_params:,}\")\nprint(f\"  Trainable parameters: {trainable_params:,}\")\nprint(f\"  Model size: ~{total_params * 4 / 1024**2:.2f} MB (float32)\")\nprint(f\"  Model size: ~{total_params * 2 / 1024**2:.2f} MB (float16/mixed precision)\")\nprint(f\"\\n✓ Ready to train with PixelShuffle architecture (128×128 → 256×256)\")"
  },
  {
   "cell_type": "markdown",
   "id": "train_funcs_header",
   "metadata": {},
   "source": [
    "## 8. Training and Validation Functions\n",
    "\n",
    "### train_epoch()\n",
    "Trains the model for one complete epoch with mixed precision:\n",
    "1. Set model to training mode\n",
    "2. For each batch:\n",
    "   - Forward pass with autocast (float16)\n",
    "   - Calculate L1 loss\n",
    "   - Backward pass with gradient scaling\n",
    "   - Update weights\n",
    "3. Return average loss\n",
    "\n",
    "### validate()\n",
    "Evaluates model on validation set:\n",
    "1. Set model to eval mode\n",
    "2. Disable gradients for faster inference\n",
    "3. Calculate average loss\n",
    "4. Return validation loss\n",
    "\n",
    "**Memory Management:**\n",
    "- Clears GPU cache between epochs\n",
    "- Uses `zero_grad(set_to_none=True)` for efficient memory cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_funcs",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, scaler, device):\n",
    "    \"\"\"Train for one epoch with mixed precision\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc='Training')\n",
    "    for lr_imgs, hr_imgs in pbar:\n",
    "        lr_imgs = lr_imgs.to(device, non_blocking=True)\n",
    "        hr_imgs = hr_imgs.to(device, non_blocking=True)\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        # Mixed precision forward pass\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            outputs = model(lr_imgs)\n",
    "            loss = criterion(outputs, hr_imgs)\n",
    "        \n",
    "        # Scaled backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.6f}'})\n",
    "    \n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate model performance\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for lr_imgs, hr_imgs in val_loader:\n",
    "            lr_imgs = lr_imgs.to(device, non_blocking=True)\n",
    "            hr_imgs = hr_imgs.to(device, non_blocking=True)\n",
    "            \n",
    "            with torch.amp.autocast('cuda'):\n",
    "                outputs = model(lr_imgs)\n",
    "                loss = criterion(outputs, hr_imgs)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "    \n",
    "    return running_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train_loop_header",
   "metadata": {},
   "source": [
    "## 9. Training Loop\n",
    "\n",
    "Main training loop that runs for all epochs.\n",
    "\n",
    "### Process:\n",
    "1. **Clear GPU cache** at start of each epoch\n",
    "2. **Train** on training set\n",
    "3. **Validate** on validation set\n",
    "4. **Record** losses to history\n",
    "5. **Save checkpoints**:\n",
    "   - Best model (lowest validation loss)\n",
    "   - Periodic checkpoints every 10 epochs\n",
    "\n",
    "### What to Monitor:\n",
    "- Training loss should decrease steadily\n",
    "- Validation loss should track training loss\n",
    "- Large gap indicates overfitting\n",
    "- GPU memory usage (printed after each epoch)\n",
    "\n",
    "**Training Time:**\n",
    "- ConvKAN is slower than standard CNN due to spline computations\n",
    "- Expect ~30-60 seconds per epoch (GPU-dependent)\n",
    "- Total training: ~30-50 minutes for 50 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_loop",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': []\n",
    "}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Starting ConvKAN Training\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Clear GPU cache\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, scaler, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Record\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nTrain Loss: {train_loss:.6f}\")\n",
    "    print(f\"Val Loss:   {val_loss:.6f}\")\n",
    "    \n",
    "    # GPU memory stats\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU Memory: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB peak\")\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'train_loss': train_loss,\n",
    "        }, CHECKPOINT_DIR / 'best_model.pth')\n",
    "        print(f\"✓ Saved best model (val_loss: {val_loss:.6f})\")\n",
    "    \n",
    "    # Periodic checkpoint\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "        }, CHECKPOINT_DIR / f'checkpoint_epoch_{epoch+1}.pth')\n",
    "        print(f\"✓ Saved checkpoint: epoch_{epoch+1}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Complete!\")\n",
    "print(f\"Best validation loss: {best_val_loss:.6f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plot_header",
   "metadata": {},
   "source": [
    "## 10. Training Curve Visualization\n",
    "\n",
    "Plot training and validation loss curves over all epochs.\n",
    "\n",
    "**Analysis:**\n",
    "- **Decreasing trend**: Both curves should trend downward\n",
    "- **Convergence**: Curves should flatten near the end\n",
    "- **Overfitting**: Val loss increasing while train loss decreases\n",
    "- **Underfitting**: Both losses remain high\n",
    "\n",
    "The plot is saved for documentation and model comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history['train_loss'], label='Training Loss', marker='o', alpha=0.7)\n",
    "plt.plot(history['val_loss'], label='Validation Loss', marker='s', alpha=0.7)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (L1)')\n",
    "plt.title('ConvKAN Training Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(CHECKPOINT_DIR / 'training_curve.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training curve saved: {CHECKPOINT_DIR / 'training_curve.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary_header",
   "metadata": {},
   "source": [
    "## 11. Training Summary\n",
    "\n",
    "Generate and save a comprehensive training summary including:\n",
    "- Model architecture details\n",
    "- Hyperparameters\n",
    "- Final performance metrics\n",
    "- File paths to saved models\n",
    "\n",
    "This summary is saved as a text file for experiment tracking and comparison with other models (e.g., U-Net)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model for final summary\n",
    "checkpoint = torch.load(CHECKPOINT_DIR / 'best_model.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "summary = f\"\"\"\n",
    "ConvKAN Super-Resolution Training Summary\n",
    "{'='*60}\n",
    "\n",
    "Model Architecture:\n",
    "  Type: ConvKAN with Residual Blocks\n",
    "  Base Filters: {BASE_FILTERS}\n",
    "  Residual Blocks: {N_RES_BLOCKS}\n",
    "  Total Parameters: {total_params:,}\n",
    "\n",
    "Training Configuration:\n",
    "  Epochs: {NUM_EPOCHS}\n",
    "  Batch Size: {BATCH_SIZE}\n",
    "  Learning Rate: {LEARNING_RATE}\n",
    "  Loss Function: L1 Loss (MAE)\n",
    "  Optimizer: AdamW with weight decay\n",
    "  Mixed Precision: Enabled\n",
    "\n",
    "Dataset:\n",
    "  Total Images: {len(full_dataset)}\n",
    "  Training: {train_size} images\n",
    "  Validation: {val_size} images\n",
    "  Input: 64×64 LR → bicubic 256×256\n",
    "  Output: 256×256 HR\n",
    "\n",
    "Final Results:\n",
    "  Best Validation Loss: {best_val_loss:.6f}\n",
    "  Best Epoch: {checkpoint['epoch'] + 1}\n",
    "  Final Train Loss: {checkpoint['train_loss']:.6f}\n",
    "\n",
    "Saved Files:\n",
    "  Best Model: {CHECKPOINT_DIR / 'best_model.pth'}\n",
    "  Training Curve: {CHECKPOINT_DIR / 'training_curve.png'}\n",
    "  \n",
    "Notes:\n",
    "  - Model processes bicubic-upsampled images (same as UNet)\n",
    "  - No PixelShuffle - constant 256×256 resolution\n",
    "  - Memory-optimized for GPU training\n",
    "  - Ready for evaluation with evaluate_convkan.ipynb\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "with open(CHECKPOINT_DIR / 'training_summary.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print(f\"\\nSummary saved: {CHECKPOINT_DIR / 'training_summary.txt'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next_steps_header",
   "metadata": {},
   "source": [
    "## 12. Next Steps\n",
    "\n",
    "Training is complete! To evaluate your model:\n",
    "\n",
    "1. **Run the evaluation notebook**: `evaluate_convkan.ipynb`\n",
    "   - Calculates PSNR and SSIM metrics\n",
    "   - Generates visual comparisons (LR vs SR vs HR)\n",
    "   - Side-by-side comparison with U-Net results\n",
    "\n",
    "2. **Compare with U-Net**:\n",
    "   - Check if ConvKAN achieves better PSNR/SSIM\n",
    "   - Analyze visual quality differences\n",
    "   - Consider training time vs performance trade-off\n",
    "\n",
    "3. **Further Improvements** (if needed):\n",
    "   - Increase `BASE_FILTERS` to 32 or 64 (if GPU memory allows)\n",
    "   - Add more residual blocks (`N_RES_BLOCKS = 6-8`)\n",
    "   - Train for more epochs (100+)\n",
    "   - Try learning rate scheduling\n",
    "   - Experiment with perceptual loss instead of L1\n",
    "\n",
    "**Model saved at:** `checkpoints_convkan/best_model.pth`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}