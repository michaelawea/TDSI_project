{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# ConvKAN-Based Image Super-Resolution Training\n",
    "\n",
    "This notebook implements a Convolutional Kolmogorov-Arnold Network (ConvKAN) architecture for image super-resolution. ConvKAN replaces traditional convolutional layers with learnable activation functions based on splines, potentially offering better feature learning capabilities.\n",
    "\n",
    "## Project Overview\n",
    "- **Task**: Image Super-Resolution - enhance bicubic-upsampled images\n",
    "- **Architecture**: ConvKAN with residual blocks\n",
    "- **Input**: Low-resolution images (64×64) upsampled to 256×256 via bicubic interpolation\n",
    "- **Output**: High-resolution images at 256×256 with restored details\n",
    "- **Loss Function**: L1 Loss (Mean Absolute Error)\n",
    "\n",
    "## Key Differences from U-Net\n",
    "- Uses ConvKAN layers instead of standard Conv2d\n",
    "- Employs residual connections for deeper feature learning\n",
    "- No downsampling - operates at constant 256×256 resolution\n",
    "- Focuses on detail restoration rather than spatial transformation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports_header",
   "metadata": {},
   "source": [
    "## 1. Import Dependencies and Environment Setup\n",
    "\n",
    "### GPU Configuration\n",
    "- `CUDA_VISIBLE_DEVICES`: Select which GPU to use (default: GPU 1)\n",
    "- `PYTORCH_CUDA_ALLOC_CONF`: Reduce memory fragmentation with expandable segments\n",
    "\n",
    "### Key Libraries\n",
    "- **convkan**: The ConvKAN layer implementation\n",
    "- **torch**: PyTorch deep learning framework\n",
    "- **PIL**: Image loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Select GPU (change to '0' for GPU0, or remove line to use default)\n",
    "os.environ.setdefault('CUDA_VISIBLE_DEVICES', '1')\n",
    "# Reduce GPU memory fragmentation\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# Import ConvKAN - install via: pip install convkan\n",
    "try:\n",
    "    from convkan import ConvKAN, LayerNorm2D\n",
    "    print(\"✓ ConvKAN imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"ERROR: convkan is not installed. Run: pip install convkan\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "device_header",
   "metadata": {},
   "source": [
    "## 2. Device Configuration\n",
    "\n",
    "Check CUDA availability and display GPU information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "device",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset_header",
   "metadata": {},
   "source": [
    "## 3. Dataset Class\n",
    "\n",
    "### SRDataset: Super-Resolution Dataset\n",
    "\n",
    "This dataset follows the **same data processing strategy as the U-Net model**:\n",
    "\n",
    "**Data Flow:**\n",
    "1. Load LR image from disk (64×64)\n",
    "2. Load corresponding HR image (256×256)\n",
    "3. **Upsample LR to 256×256 using bicubic interpolation** ← Key step!\n",
    "4. Convert both to tensors\n",
    "5. Return (LR_upsampled, HR) pair\n",
    "\n",
    "**Why Bicubic Upsampling?**\n",
    "- The model's job is to **refine** the bicubic result, not perform raw upscaling\n",
    "- This is a more realistic task: starting from a decent baseline and adding details\n",
    "- Matches the UNet approach for fair comparison\n",
    "\n",
    "**Important Notes:**\n",
    "- Both LR and HR are at 256×256 when fed to the model\n",
    "- The model learns to transform blurry bicubic → sharp HR\n",
    "- No PixelShuffle needed - we work at constant resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRDataset(Dataset):\n",
    "    \"\"\"Super-Resolution Dataset - matches UNet data processing\"\"\"\n",
    "    def __init__(self, hr_dir, lr_dir, hr_size=256, transform=None):\n",
    "        self.hr_dir = Path(hr_dir)\n",
    "        self.lr_dir = Path(lr_dir)\n",
    "        self.hr_size = hr_size\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Load all image file paths\n",
    "        self.hr_images = sorted(list(self.hr_dir.glob('*.png')))\n",
    "        self.lr_images = sorted(list(self.lr_dir.glob('*.png')))\n",
    "        \n",
    "        # Verify dataset integrity\n",
    "        if not self.hr_images or not self.lr_images:\n",
    "            raise IOError(f\"No images found in {hr_dir} or {lr_dir}\")\n",
    "        \n",
    "        assert len(self.hr_images) == len(self.lr_images), \\\n",
    "            f\"Mismatch: {len(self.hr_images)} HR vs {len(self.lr_images)} LR images\"\n",
    "        \n",
    "        print(f\"Dataset loaded: {len(self.hr_images)} image pairs\")\n",
    "        \n",
    "        # Check actual image sizes\n",
    "        sample_lr = Image.open(self.lr_images[0])\n",
    "        sample_hr = Image.open(self.hr_images[0])\n",
    "        print(f\"Original LR size: {sample_lr.size}\")\n",
    "        print(f\"Original HR size: {sample_hr.size}\")\n",
    "        print(f\"LR will be upsampled to: {hr_size}×{hr_size} (bicubic)\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.hr_images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load images\n",
    "        hr_img = Image.open(self.hr_images[idx]).convert('RGB')\n",
    "        lr_img = Image.open(self.lr_images[idx]).convert('RGB')\n",
    "        \n",
    "        # CRITICAL: Upsample LR to HR size using bicubic (same as UNet)\n",
    "        lr_img = lr_img.resize((self.hr_size, self.hr_size), Image.BICUBIC)\n",
    "        \n",
    "        # Apply transforms (to tensor)\n",
    "        if self.transform:\n",
    "            hr_img = self.transform(hr_img)\n",
    "            lr_img = self.transform(lr_img)\n",
    "        \n",
    "        return lr_img, hr_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_header",
   "metadata": {},
   "source": [
    "## 4. ConvKAN Model Architecture\n",
    "\n",
    "### What is ConvKAN?\n",
    "\n",
    "ConvKAN (Convolutional Kolmogorov-Arnold Network) replaces traditional convolutional layers with learnable spline-based activation functions. Based on the Kolmogorov-Arnold representation theorem, it can potentially learn more complex feature transformations.\n",
    "\n",
    "**Key Differences from Standard CNNs:**\n",
    "- Uses B-spline basis functions for activation\n",
    "- Learnable activation curves (vs fixed ReLU/sigmoid)\n",
    "- More parameters but potentially better expressiveness\n",
    "\n",
    "---\n",
    "\n",
    "### 4.1 ConvKANResBlock\n",
    "\n",
    "A residual block using ConvKAN layers:\n",
    "```\n",
    "Input → ConvKAN → LayerNorm → ConvKAN → LayerNorm → (+) Input → Output\n",
    "```\n",
    "\n",
    "**Components:**\n",
    "- **ConvKAN(3×3)**: Convolutional KAN layer with learnable activations\n",
    "- **LayerNorm2D**: Normalization for stable training\n",
    "- **Residual connection**: Allows gradient flow and feature reuse\n",
    "\n",
    "---\n",
    "\n",
    "### 4.2 ConvKAN_SR: Full Model Architecture\n",
    "\n",
    "A compact architecture designed for detail restoration:\n",
    "\n",
    "```\n",
    "Input: 3 channels (RGB), 256×256\n",
    "    ↓ [Head: ConvKAN 3→base_filters]\n",
    "  base_filters channels, 256×256\n",
    "    ↓ [Body: n_res_blocks × ConvKANResBlock] ← Residual connection\n",
    "  base_filters channels, 256×256\n",
    "    ↓ [Tail: Conv2d 1×1]\n",
    "Output: 3 channels (RGB), 256×256\n",
    "```\n",
    "\n",
    "**Architecture Design:**\n",
    "1. **Head**: Initial feature extraction from bicubic-upsampled input\n",
    "2. **Body**: Multiple residual blocks for deep feature learning\n",
    "   - Long skip connection from head to output\n",
    "   - Each block has internal skip connections\n",
    "3. **Tail**: Final 1×1 conv to map features back to RGB\n",
    "\n",
    "**Key Parameters:**\n",
    "- `base_filters`: Feature channel width (16-64 for memory efficiency)\n",
    "- `n_res_blocks`: Number of residual blocks (2-8 typical)\n",
    "- **No upsampling layers** - operates at constant 256×256 resolution\n",
    "\n",
    "**Memory Optimization:**\n",
    "- Reduced base_filters (16) to fit in GPU memory\n",
    "- Fewer residual blocks (2-4) for faster training\n",
    "- ConvKAN layers are memory-intensive due to spline computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvKANResBlock(nn.Module):\n",
    "    \"\"\"Residual block with ConvKAN layers\"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            ConvKAN(channels, channels, kernel_size=3, padding=1),\n",
    "            LayerNorm2D(channels),\n",
    "            ConvKAN(channels, channels, kernel_size=3, padding=1),\n",
    "            LayerNorm2D(channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)  # Residual connection\n",
    "\n",
    "\n",
    "class ConvKAN_SR(nn.Module):\n",
    "    \"\"\"\n",
    "    ConvKAN Super-Resolution Model\n",
    "    \n",
    "    Architecture: Head → Body (Residual Blocks) → Tail\n",
    "    No upsampling - works at constant 256×256 resolution\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=3, out_channels=3, base_filters=16, n_res_blocks=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Head: Initial feature extraction\n",
    "        self.head = ConvKAN(in_channels, base_filters, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Body: Deep feature learning with residual blocks\n",
    "        body = [ConvKANResBlock(base_filters) for _ in range(n_res_blocks)]\n",
    "        self.body = nn.Sequential(*body)\n",
    "        \n",
    "        # Tail: Map features back to RGB\n",
    "        self.tail = nn.Conv2d(base_filters, out_channels, kernel_size=3, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Extract initial features\n",
    "        x = self.head(x)\n",
    "        \n",
    "        # Deep feature extraction with long skip connection\n",
    "        res = x\n",
    "        x = self.body(x)\n",
    "        x = x + res  # Long residual connection\n",
    "        \n",
    "        # Reconstruct RGB image\n",
    "        x = self.tail(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config_header",
   "metadata": {},
   "source": [
    "## 5. Training Configuration\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "**Memory-Optimized Settings:**\n",
    "- **BATCH_SIZE = 4**: Small batch to fit ConvKAN in GPU memory\n",
    "- **BASE_FILTERS = 16**: Reduced from typical 64 to save memory\n",
    "- **N_RES_BLOCKS = 4**: Fewer blocks for faster iteration\n",
    "- **LEARNING_RATE = 1e-4**: Conservative rate for stable training\n",
    "- **NUM_EPOCHS = 50**: Enough for convergence\n",
    "\n",
    "**Data Split:**\n",
    "- 90% training, 10% validation (same as UNet)\n",
    "\n",
    "**Note:** ConvKAN is more memory-intensive than standard Conv2d due to spline computations. If you encounter OOM errors, reduce BATCH_SIZE to 2 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters - optimized for GPU memory\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 50\n",
    "TRAIN_SPLIT = 0.9\n",
    "\n",
    "# Model architecture\n",
    "BASE_FILTERS = 16  # Reduced to save memory\n",
    "N_RES_BLOCKS = 4   # Fewer blocks for faster training\n",
    "\n",
    "# Data paths\n",
    "HR_DIR = './dataset/high_resolution'\n",
    "LR_DIR = './dataset/low_resolution'\n",
    "CHECKPOINT_DIR = Path('./checkpoints_convkan')\n",
    "CHECKPOINT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Model: base_filters={BASE_FILTERS}, n_res_blocks={N_RES_BLOCKS}\")\n",
    "print(f\"Strategy: Memory-optimized ConvKAN for detail restoration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_header",
   "metadata": {},
   "source": [
    "## 6. Data Loading and Preprocessing\n",
    "\n",
    "### Data Pipeline:\n",
    "1. **Transform**: Convert PIL images to tensors (range [0, 1])\n",
    "2. **Dataset**: Load with bicubic upsampling (LR 64×64 → 256×256)\n",
    "3. **Split**: 90% train, 10% validation\n",
    "4. **DataLoader**: \n",
    "   - `num_workers=0` to avoid multiprocessing issues in notebooks\n",
    "   - `pin_memory=True` for faster GPU transfer\n",
    "\n",
    "**Expected Data:**\n",
    "- 1000 image pairs (typical)\n",
    "- LR: 64×64 PNG files\n",
    "- HR: 256×256 PNG files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform: to tensor only\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load dataset (bicubic upsampling happens inside dataset)\n",
    "full_dataset = SRDataset(HR_DIR, LR_DIR, hr_size=256, transform=transform)\n",
    "\n",
    "# Split train/val\n",
    "train_size = int(TRAIN_SPLIT * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    full_dataset, [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)  # Reproducible split\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=0,  # Avoid multiprocessing issues\n",
    "    pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset split:\")\n",
    "print(f\"  Training: {train_size} images ({len(train_loader)} batches)\")\n",
    "print(f\"  Validation: {val_size} images ({len(val_loader)} batches)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "init_header",
   "metadata": {},
   "source": [
    "## 7. Model Initialization\n",
    "\n",
    "### Components:\n",
    "- **Model**: ConvKAN_SR with reduced filters for memory efficiency\n",
    "- **Loss Function**: L1 Loss (MAE) - preserves sharp edges better than MSE\n",
    "- **Optimizer**: AdamW with weight decay for regularization\n",
    "- **Mixed Precision**: GradScaler for faster training with float16\n",
    "\n",
    "**Parameter Count:**\n",
    "ConvKAN has more parameters than standard Conv2d due to learnable spline coefficients. The model will display total trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "init",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = ConvKAN_SR(\n",
    "    in_channels=3, \n",
    "    out_channels=3, \n",
    "    base_filters=BASE_FILTERS, \n",
    "    n_res_blocks=N_RES_BLOCKS\n",
    ").to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "\n",
    "# Mixed precision training\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "# Print model info\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nModel Statistics:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Model size: ~{total_params * 4 / 1024**2:.2f} MB (float32)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train_funcs_header",
   "metadata": {},
   "source": [
    "## 8. Training and Validation Functions\n",
    "\n",
    "### train_epoch()\n",
    "Trains the model for one complete epoch with mixed precision:\n",
    "1. Set model to training mode\n",
    "2. For each batch:\n",
    "   - Forward pass with autocast (float16)\n",
    "   - Calculate L1 loss\n",
    "   - Backward pass with gradient scaling\n",
    "   - Update weights\n",
    "3. Return average loss\n",
    "\n",
    "### validate()\n",
    "Evaluates model on validation set:\n",
    "1. Set model to eval mode\n",
    "2. Disable gradients for faster inference\n",
    "3. Calculate average loss\n",
    "4. Return validation loss\n",
    "\n",
    "**Memory Management:**\n",
    "- Clears GPU cache between epochs\n",
    "- Uses `zero_grad(set_to_none=True)` for efficient memory cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_funcs",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, scaler, device):\n",
    "    \"\"\"Train for one epoch with mixed precision\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc='Training')\n",
    "    for lr_imgs, hr_imgs in pbar:\n",
    "        lr_imgs = lr_imgs.to(device, non_blocking=True)\n",
    "        hr_imgs = hr_imgs.to(device, non_blocking=True)\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        # Mixed precision forward pass\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            outputs = model(lr_imgs)\n",
    "            loss = criterion(outputs, hr_imgs)\n",
    "        \n",
    "        # Scaled backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.6f}'})\n",
    "    \n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate model performance\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for lr_imgs, hr_imgs in val_loader:\n",
    "            lr_imgs = lr_imgs.to(device, non_blocking=True)\n",
    "            hr_imgs = hr_imgs.to(device, non_blocking=True)\n",
    "            \n",
    "            with torch.amp.autocast('cuda'):\n",
    "                outputs = model(lr_imgs)\n",
    "                loss = criterion(outputs, hr_imgs)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "    \n",
    "    return running_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train_loop_header",
   "metadata": {},
   "source": [
    "## 9. Training Loop\n",
    "\n",
    "Main training loop that runs for all epochs.\n",
    "\n",
    "### Process:\n",
    "1. **Clear GPU cache** at start of each epoch\n",
    "2. **Train** on training set\n",
    "3. **Validate** on validation set\n",
    "4. **Record** losses to history\n",
    "5. **Save checkpoints**:\n",
    "   - Best model (lowest validation loss)\n",
    "   - Periodic checkpoints every 10 epochs\n",
    "\n",
    "### What to Monitor:\n",
    "- Training loss should decrease steadily\n",
    "- Validation loss should track training loss\n",
    "- Large gap indicates overfitting\n",
    "- GPU memory usage (printed after each epoch)\n",
    "\n",
    "**Training Time:**\n",
    "- ConvKAN is slower than standard CNN due to spline computations\n",
    "- Expect ~30-60 seconds per epoch (GPU-dependent)\n",
    "- Total training: ~30-50 minutes for 50 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_loop",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': []\n",
    "}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Starting ConvKAN Training\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Clear GPU cache\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, scaler, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Record\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nTrain Loss: {train_loss:.6f}\")\n",
    "    print(f\"Val Loss:   {val_loss:.6f}\")\n",
    "    \n",
    "    # GPU memory stats\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU Memory: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB peak\")\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'train_loss': train_loss,\n",
    "        }, CHECKPOINT_DIR / 'best_model.pth')\n",
    "        print(f\"✓ Saved best model (val_loss: {val_loss:.6f})\")\n",
    "    \n",
    "    # Periodic checkpoint\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "        }, CHECKPOINT_DIR / f'checkpoint_epoch_{epoch+1}.pth')\n",
    "        print(f\"✓ Saved checkpoint: epoch_{epoch+1}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Complete!\")\n",
    "print(f\"Best validation loss: {best_val_loss:.6f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plot_header",
   "metadata": {},
   "source": [
    "## 10. Training Curve Visualization\n",
    "\n",
    "Plot training and validation loss curves over all epochs.\n",
    "\n",
    "**Analysis:**\n",
    "- **Decreasing trend**: Both curves should trend downward\n",
    "- **Convergence**: Curves should flatten near the end\n",
    "- **Overfitting**: Val loss increasing while train loss decreases\n",
    "- **Underfitting**: Both losses remain high\n",
    "\n",
    "The plot is saved for documentation and model comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history['train_loss'], label='Training Loss', marker='o', alpha=0.7)\n",
    "plt.plot(history['val_loss'], label='Validation Loss', marker='s', alpha=0.7)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (L1)')\n",
    "plt.title('ConvKAN Training Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(CHECKPOINT_DIR / 'training_curve.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training curve saved: {CHECKPOINT_DIR / 'training_curve.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary_header",
   "metadata": {},
   "source": [
    "## 11. Training Summary\n",
    "\n",
    "Generate and save a comprehensive training summary including:\n",
    "- Model architecture details\n",
    "- Hyperparameters\n",
    "- Final performance metrics\n",
    "- File paths to saved models\n",
    "\n",
    "This summary is saved as a text file for experiment tracking and comparison with other models (e.g., U-Net)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model for final summary\n",
    "checkpoint = torch.load(CHECKPOINT_DIR / 'best_model.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "summary = f\"\"\"\n",
    "ConvKAN Super-Resolution Training Summary\n",
    "{'='*60}\n",
    "\n",
    "Model Architecture:\n",
    "  Type: ConvKAN with Residual Blocks\n",
    "  Base Filters: {BASE_FILTERS}\n",
    "  Residual Blocks: {N_RES_BLOCKS}\n",
    "  Total Parameters: {total_params:,}\n",
    "\n",
    "Training Configuration:\n",
    "  Epochs: {NUM_EPOCHS}\n",
    "  Batch Size: {BATCH_SIZE}\n",
    "  Learning Rate: {LEARNING_RATE}\n",
    "  Loss Function: L1 Loss (MAE)\n",
    "  Optimizer: AdamW with weight decay\n",
    "  Mixed Precision: Enabled\n",
    "\n",
    "Dataset:\n",
    "  Total Images: {len(full_dataset)}\n",
    "  Training: {train_size} images\n",
    "  Validation: {val_size} images\n",
    "  Input: 64×64 LR → bicubic 256×256\n",
    "  Output: 256×256 HR\n",
    "\n",
    "Final Results:\n",
    "  Best Validation Loss: {best_val_loss:.6f}\n",
    "  Best Epoch: {checkpoint['epoch'] + 1}\n",
    "  Final Train Loss: {checkpoint['train_loss']:.6f}\n",
    "\n",
    "Saved Files:\n",
    "  Best Model: {CHECKPOINT_DIR / 'best_model.pth'}\n",
    "  Training Curve: {CHECKPOINT_DIR / 'training_curve.png'}\n",
    "  \n",
    "Notes:\n",
    "  - Model processes bicubic-upsampled images (same as UNet)\n",
    "  - No PixelShuffle - constant 256×256 resolution\n",
    "  - Memory-optimized for GPU training\n",
    "  - Ready for evaluation with evaluate_convkan.ipynb\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "with open(CHECKPOINT_DIR / 'training_summary.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print(f\"\\nSummary saved: {CHECKPOINT_DIR / 'training_summary.txt'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next_steps_header",
   "metadata": {},
   "source": [
    "## 12. Next Steps\n",
    "\n",
    "Training is complete! To evaluate your model:\n",
    "\n",
    "1. **Run the evaluation notebook**: `evaluate_convkan.ipynb`\n",
    "   - Calculates PSNR and SSIM metrics\n",
    "   - Generates visual comparisons (LR vs SR vs HR)\n",
    "   - Side-by-side comparison with U-Net results\n",
    "\n",
    "2. **Compare with U-Net**:\n",
    "   - Check if ConvKAN achieves better PSNR/SSIM\n",
    "   - Analyze visual quality differences\n",
    "   - Consider training time vs performance trade-off\n",
    "\n",
    "3. **Further Improvements** (if needed):\n",
    "   - Increase `BASE_FILTERS` to 32 or 64 (if GPU memory allows)\n",
    "   - Add more residual blocks (`N_RES_BLOCKS = 6-8`)\n",
    "   - Train for more epochs (100+)\n",
    "   - Try learning rate scheduling\n",
    "   - Experiment with perceptual loss instead of L1\n",
    "\n",
    "**Model saved at:** `checkpoints_convkan/best_model.pth`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
