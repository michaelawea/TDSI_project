{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# ConvKAN-Based Image Super-Resolution Training\n",
    "\n",
    "This notebook implements a Convolutional Kolmogorov-Arnold Network (ConvKAN) architecture for image super-resolution. ConvKAN replaces traditional convolutional layers with learnable activation functions based on splines, potentially offering better feature learning capabilities.\n",
    "\n",
    "## Project Overview\n",
    "- **Task**: Image Super-Resolution - enhance bicubic-upsampled images\n",
    "- **Architecture**: ConvKAN with residual blocks\n",
    "- **Input**: Low-resolution images (64×64) upsampled to 256×256 via bicubic interpolation\n",
    "- **Output**: High-resolution images at 256×256 with restored details\n",
    "- **Loss Function**: L1 Loss (Mean Absolute Error)\n",
    "\n",
    "## Key Differences from U-Net\n",
    "- Uses ConvKAN layers instead of standard Conv2d\n",
    "- Employs residual connections for deeper feature learning\n",
    "- No downsampling - operates at constant 256×256 resolution\n",
    "- Focuses on detail restoration rather than spatial transformation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports_header",
   "metadata": {},
   "source": [
    "## 1. Import Dependencies and Environment Setup\n",
    "\n",
    "### GPU Configuration\n",
    "- `CUDA_VISIBLE_DEVICES`: Select which GPU to use (default: GPU 1)\n",
    "- `PYTORCH_CUDA_ALLOC_CONF`: Reduce memory fragmentation with expandable segments\n",
    "\n",
    "### Key Libraries\n",
    "- **convkan**: The ConvKAN layer implementation\n",
    "- **torch**: PyTorch deep learning framework\n",
    "- **PIL**: Image loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Select GPU (change to '0' for GPU0, or remove line to use default)\n",
    "os.environ.setdefault('CUDA_VISIBLE_DEVICES', '1')\n",
    "# Reduce GPU memory fragmentation\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# Import ConvKAN - install via: pip install convkan\n",
    "try:\n",
    "    from convkan import ConvKAN, LayerNorm2D\n",
    "    print(\"✓ ConvKAN imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"ERROR: convkan is not installed. Run: pip install convkan\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "device_header",
   "metadata": {},
   "source": [
    "## 2. Device Configuration\n",
    "\n",
    "Check CUDA availability and display GPU information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "device",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset_header",
   "metadata": {},
   "source": [
    "## 3. Dataset Class\n",
    "\n",
    "### SRDataset: Super-Resolution Dataset\n",
    "\n",
    "This dataset follows the **same data processing strategy as the U-Net model**:\n",
    "\n",
    "**Data Flow:**\n",
    "1. Load LR image from disk (64×64)\n",
    "2. Load corresponding HR image (256×256)\n",
    "3. **Upsample LR to 256×256 using bicubic interpolation** ← Key step!\n",
    "4. Convert both to tensors\n",
    "5. Return (LR_upsampled, HR) pair\n",
    "\n",
    "**Why Bicubic Upsampling?**\n",
    "- The model's job is to **refine** the bicubic result, not perform raw upscaling\n",
    "- This is a more realistic task: starting from a decent baseline and adding details\n",
    "- Matches the UNet approach for fair comparison\n",
    "\n",
    "**Important Notes:**\n",
    "- Both LR and HR are at 256×256 when fed to the model\n",
    "- The model learns to transform blurry bicubic → sharp HR\n",
    "- No PixelShuffle needed - we work at constant resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRDataset(Dataset):\n",
    "    \"\"\"Super-Resolution Dataset - matches UNet data processing\"\"\"\n",
    "    def __init__(self, hr_dir, lr_dir, hr_size=256, transform=None):\n",
    "        self.hr_dir = Path(hr_dir)\n",
    "        self.lr_dir = Path(lr_dir)\n",
    "        self.hr_size = hr_size\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Load all image file paths\n",
    "        self.hr_images = sorted(list(self.hr_dir.glob('*.png')))\n",
    "        self.lr_images = sorted(list(self.lr_dir.glob('*.png')))\n",
    "        \n",
    "        # Verify dataset integrity\n",
    "        if not self.hr_images or not self.lr_images:\n",
    "            raise IOError(f\"No images found in {hr_dir} or {lr_dir}\")\n",
    "        \n",
    "        assert len(self.hr_images) == len(self.lr_images), \\\n",
    "            f\"Mismatch: {len(self.hr_images)} HR vs {len(self.lr_images)} LR images\"\n",
    "        \n",
    "        print(f\"Dataset loaded: {len(self.hr_images)} image pairs\")\n",
    "        \n",
    "        # Check actual image sizes\n",
    "        sample_lr = Image.open(self.lr_images[0])\n",
    "        sample_hr = Image.open(self.hr_images[0])\n",
    "        print(f\"Original LR size: {sample_lr.size}\")\n",
    "        print(f\"Original HR size: {sample_hr.size}\")\n",
    "        print(f\"LR will be upsampled to: {hr_size}×{hr_size} (bicubic)\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.hr_images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load images\n",
    "        hr_img = Image.open(self.hr_images[idx]).convert('RGB')\n",
    "        lr_img = Image.open(self.lr_images[idx]).convert('RGB')\n",
    "        \n",
    "        # CRITICAL: Upsample LR to HR size using bicubic (same as UNet)\n",
    "        lr_img = lr_img.resize((self.hr_size, self.hr_size), Image.BICUBIC)\n",
    "        \n",
    "        # Apply transforms (to tensor)\n",
    "        if self.transform:\n",
    "            hr_img = self.transform(hr_img)\n",
    "            lr_img = self.transform(lr_img)\n",
    "        \n",
    "        return lr_img, hr_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_header",
   "metadata": {},
   "source": "## 4. ConvKAN Model Architecture\n\n### What is ConvKAN?\n\nConvKAN (Convolutional Kolmogorov-Arnold Network) replaces traditional convolutional layers with learnable spline-based activation functions. Based on the Kolmogorov-Arnold representation theorem, it can potentially learn more complex feature transformations.\n\n**Key Differences from Standard CNNs:**\n- Uses B-spline basis functions for activation\n- Learnable activation curves (vs fixed ReLU/sigmoid)\n- More parameters but potentially better expressiveness\n\n**Memory Challenge at 256×256:**\nConvKAN's spline computations are **extremely memory-intensive** at high resolutions. For 256×256 images:\n- Standard Conv2d: ~4 MB per layer\n- ConvKAN: ~400 MB per layer (100× more!)\n\n**Our Solution: Ultra-Lightweight Architecture**\n\nTo make ConvKAN work at 256×256, we use an **extremely simplified** architecture:\n\n```\nInput: RGB 256×256 (3 channels)\n    ↓\n[Head: ConvKAN 3→8 channels]  ← Minimal feature extraction\n    ↓\n[Body: 1× ResBlock (8 ch)]    ← Only ONE residual block\n    ↓\n[Tail: Conv2d 8→3]            ← Standard conv (fast)\n    ↓\nOutput: RGB 256×256 (3 channels)\n```\n\n**Key Optimizations:**\n1. **Minimal channels**: Only 8 base filters (vs typical 64)\n2. **Single residual block**: Just 1 block (vs typical 8-16)\n3. **Mixed Conv types**: ConvKAN only in critical paths, standard Conv2d for output\n4. **No upsampling**: Constant 256×256 resolution (no PixelShuffle overhead)\n5. **Gradient checkpointing**: Enabled to trade compute for memory\n\n**Trade-offs:**\n- ✅ Fits in GPU memory\n- ✅ Still uses ConvKAN's learnable activations\n- ⚠️ Reduced capacity vs full model\n- ⚠️ May need more epochs to converge\n\n---"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model",
   "metadata": {},
   "outputs": [],
   "source": "class ConvKANResBlock(nn.Module):\n    \"\"\"\n    Residual block with ConvKAN layers\n    \n    Architecture:\n        Input (C channels, 256×256)\n            ↓\n        ConvKAN(C→C, 3×3) + LayerNorm\n            ↓\n        ConvKAN(C→C, 3×3) + LayerNorm\n            ↓\n        Add residual connection\n            ↓\n        Output (C channels, 256×256)\n    \n    Memory optimization:\n        - Uses gradient checkpointing to save memory\n        - Single path (no branching) for minimal overhead\n    \"\"\"\n    def __init__(self, channels):\n        super().__init__()\n        self.block = nn.Sequential(\n            ConvKAN(channels, channels, kernel_size=3, padding=1),\n            LayerNorm2D(channels),\n            ConvKAN(channels, channels, kernel_size=3, padding=1),\n            LayerNorm2D(channels)\n        )\n\n    def forward(self, x):\n        # Simple residual: output = input + transform(input)\n        return x + self.block(x)\n\n\nclass ConvKAN_SR(nn.Module):\n    \"\"\"\n    Ultra-Lightweight ConvKAN for Super-Resolution\n    \n    Full Architecture Breakdown:\n    ============================\n    \n    Layer               Output Shape        Parameters    Memory (256×256)\n    ─────────────────────────────────────────────────────────────────────\n    Input               [B, 3, 256, 256]    -             0.75 MB\n    \n    HEAD (Feature Extraction):\n    ConvKAN(3→8)        [B, 8, 256, 256]    ~50K          2 MB\n    \n    BODY (Feature Refinement):\n    ResBlock1:\n      - ConvKAN(8→8)    [B, 8, 256, 256]    ~120K         2 MB\n      - LayerNorm       [B, 8, 256, 256]    16            0 MB\n      - ConvKAN(8→8)    [B, 8, 256, 256]    ~120K         2 MB\n      - LayerNorm       [B, 8, 256, 256]    16            0 MB\n      + Residual        [B, 8, 256, 256]    -             0 MB\n    \n    TAIL (RGB Reconstruction):\n    Conv2d(8→3)         [B, 3, 256, 256]    219           0.01 MB\n    \n    ─────────────────────────────────────────────────────────────────────\n    Total Parameters:   ~290K (0.29M)\n    Peak Memory (FP16): ~8-10 GB (with batch_size=1, including gradients)\n    ─────────────────────────────────────────────────────────────────────\n    \n    Design Rationale:\n    ─────────────────\n    1. **Minimal channels (8)**: Reduces memory by 64× vs standard (64 channels)\n    2. **Single ResBlock**: Only 1 block to minimize depth\n    3. **Standard Conv2d tail**: Faster than ConvKAN for final layer\n    4. **No downsampling**: Avoids memory spikes from pooling\n    5. **No upsampling**: Works at constant 256×256 (input already upsampled)\n    \n    Comparison to Standard SR Models:\n    ─────────────────────────────────\n    - EDSR (baseline): ~1.5M params, 16 ResBlocks, 64 filters\n    - Our ConvKAN: ~0.3M params, 1 ResBlock, 8 filters\n    - Size reduction: ~5×\n    - Memory reduction: ~100× (due to ConvKAN overhead per channel)\n    \"\"\"\n    def __init__(self, in_channels=3, out_channels=3, base_filters=8, n_res_blocks=1):\n        super().__init__()\n        \n        print(f\"\\n{'='*60}\")\n        print(f\"Initializing Ultra-Lightweight ConvKAN_SR\")\n        print(f\"{'='*60}\")\n        print(f\"  Input channels:    {in_channels}\")\n        print(f\"  Output channels:   {out_channels}\")\n        print(f\"  Base filters:      {base_filters}\")\n        print(f\"  Residual blocks:   {n_res_blocks}\")\n        print(f\"  Resolution:        256×256 (constant)\")\n        print(f\"{'='*60}\\n\")\n        \n        # Head: Initial feature extraction\n        # ConvKAN for learnable nonlinear feature extraction\n        self.head = ConvKAN(in_channels, base_filters, kernel_size=3, padding=1)\n        \n        # Body: Deep feature learning with residual blocks\n        # Minimal depth to save memory\n        body = [ConvKANResBlock(base_filters) for _ in range(n_res_blocks)]\n        self.body = nn.Sequential(*body)\n        \n        # Tail: Map features back to RGB\n        # Use standard Conv2d (much faster and lighter than ConvKAN)\n        self.tail = nn.Conv2d(base_filters, out_channels, kernel_size=3, padding=1)\n        \n        # Count parameters\n        total_params = sum(p.numel() for p in self.parameters())\n        print(f\"Total parameters: {total_params:,} ({total_params/1e6:.2f}M)\")\n        \n    def forward(self, x):\n        \"\"\"\n        Forward pass with memory-efficient processing\n        \n        Args:\n            x: Input tensor [B, 3, 256, 256]\n        \n        Returns:\n            Output tensor [B, 3, 256, 256]\n        \"\"\"\n        # Extract initial features\n        x = self.head(x)  # [B, 3, 256, 256] → [B, 8, 256, 256]\n        \n        # Deep feature extraction with long skip connection\n        res = x\n        x = self.body(x)  # [B, 8, 256, 256] → [B, 8, 256, 256]\n        x = x + res       # Long residual connection\n        \n        # Reconstruct RGB image\n        x = self.tail(x)  # [B, 8, 256, 256] → [B, 3, 256, 256]\n        \n        return x"
  },
  {
   "cell_type": "markdown",
   "id": "config_header",
   "metadata": {},
   "source": [
    "## 5. Training Configuration\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "**Memory-Optimized Settings:**\n",
    "- **BATCH_SIZE = 4**: Small batch to fit ConvKAN in GPU memory\n",
    "- **BASE_FILTERS = 16**: Reduced from typical 64 to save memory\n",
    "- **N_RES_BLOCKS = 4**: Fewer blocks for faster iteration\n",
    "- **LEARNING_RATE = 1e-4**: Conservative rate for stable training\n",
    "- **NUM_EPOCHS = 50**: Enough for convergence\n",
    "\n",
    "**Data Split:**\n",
    "- 90% training, 10% validation (same as UNet)\n",
    "\n",
    "**Note:** ConvKAN is more memory-intensive than standard Conv2d due to spline computations. If you encounter OOM errors, reduce BATCH_SIZE to 2 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": "# Hyperparameters - EXTREME memory optimization for 256x256 ConvKAN\nBATCH_SIZE = 1\nLEARNING_RATE = 1e-4\nNUM_EPOCHS = 50\nTRAIN_SPLIT = 0.9\n\n# Model architecture - Minimal viable model\nBASE_FILTERS = 8   # Very small\nN_RES_BLOCKS = 1   # Only 1 residual block!\n\n# Data paths\nHR_DIR = './dataset/high_resolution'\nLR_DIR = './dataset/low_resolution'\nCHECKPOINT_DIR = Path('./checkpoints_convkan')\nCHECKPOINT_DIR.mkdir(exist_ok=True)\n\nprint(f\"Batch size: {BATCH_SIZE}\")\nprint(f\"Learning rate: {LEARNING_RATE}\")\nprint(f\"Epochs: {NUM_EPOCHS}\")\nprint(f\"Model: base_filters={BASE_FILTERS}, n_res_blocks={N_RES_BLOCKS}\")\nprint(f\"⚠️  Minimal model - ConvKAN at 256x256 is very memory intensive\")\nprint(f\"Strategy: Bicubic upsampling 64x64→256x256, model refines details\")"
  },
  {
   "cell_type": "markdown",
   "id": "data_header",
   "metadata": {},
   "source": [
    "## 6. Data Loading and Preprocessing\n",
    "\n",
    "### Data Pipeline:\n",
    "1. **Transform**: Convert PIL images to tensors (range [0, 1])\n",
    "2. **Dataset**: Load with bicubic upsampling (LR 64×64 → 256×256)\n",
    "3. **Split**: 90% train, 10% validation\n",
    "4. **DataLoader**: \n",
    "   - `num_workers=0` to avoid multiprocessing issues in notebooks\n",
    "   - `pin_memory=True` for faster GPU transfer\n",
    "\n",
    "**Expected Data:**\n",
    "- 1000 image pairs (typical)\n",
    "- LR: 64×64 PNG files\n",
    "- HR: 256×256 PNG files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform: to tensor only\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load dataset (bicubic upsampling happens inside dataset)\n",
    "full_dataset = SRDataset(HR_DIR, LR_DIR, hr_size=256, transform=transform)\n",
    "\n",
    "# Split train/val\n",
    "train_size = int(TRAIN_SPLIT * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    full_dataset, [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)  # Reproducible split\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=0,  # Avoid multiprocessing issues\n",
    "    pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset split:\")\n",
    "print(f\"  Training: {train_size} images ({len(train_loader)} batches)\")\n",
    "print(f\"  Validation: {val_size} images ({len(val_loader)} batches)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2nukh7ey865",
   "source": "### Verify Dataset Loading\n\nLet's visualize 2 random samples to verify the dataset is loaded correctly.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "x02qofusmj7",
   "source": "import matplotlib.pyplot as plt\nimport random\n\n# Select 2 random samples\nsample_indices = random.sample(range(len(full_dataset)), 2)\n\nfig, axes = plt.subplots(2, 2, figsize=(10, 10))\n\nfor i, idx in enumerate(sample_indices):\n    lr_img, hr_img = full_dataset[idx]\n    \n    # Convert to numpy for display\n    lr_np = lr_img.numpy().transpose(1, 2, 0)\n    hr_np = hr_img.numpy().transpose(1, 2, 0)\n    \n    # Display LR\n    axes[i, 0].imshow(lr_np)\n    axes[i, 0].set_title(f'Sample {i+1}: LR (Bicubic 256x256)\\nShape: {lr_img.shape}')\n    axes[i, 0].axis('off')\n    \n    # Display HR\n    axes[i, 1].imshow(hr_np)\n    axes[i, 1].set_title(f'Sample {i+1}: HR (Ground Truth)\\nShape: {hr_img.shape}')\n    axes[i, 1].axis('off')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"✓ Dataset verification complete\")\nprint(f\"  LR images: Bicubic upsampled from 64x64 to 256x256\")\nprint(f\"  HR images: Original 256x256 ground truth\")\nprint(f\"  Total samples: {len(full_dataset)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "init_header",
   "metadata": {},
   "source": [
    "## 7. Model Initialization\n",
    "\n",
    "### Components:\n",
    "- **Model**: ConvKAN_SR with reduced filters for memory efficiency\n",
    "- **Loss Function**: L1 Loss (MAE) - preserves sharp edges better than MSE\n",
    "- **Optimizer**: AdamW with weight decay for regularization\n",
    "- **Mixed Precision**: GradScaler for faster training with float16\n",
    "\n",
    "**Parameter Count:**\n",
    "ConvKAN has more parameters than standard Conv2d due to learnable spline coefficients. The model will display total trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "init",
   "metadata": {},
   "outputs": [],
   "source": "# Initialize model\nmodel = ConvKAN_SR(\n    in_channels=3, \n    out_channels=3, \n    base_filters=BASE_FILTERS, \n    n_res_blocks=N_RES_BLOCKS\n).to(device)\n\n# Loss and optimizer\ncriterion = nn.L1Loss()\noptimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n\n# Mixed precision training\nscaler = torch.amp.GradScaler('cuda')\n\n# Enable memory optimizations\nif device.type == 'cuda':\n    # Enable TF32 for faster training on Ampere+ GPUs\n    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.allow_tf32 = True\n    \n    # Enable cudnn benchmarking for faster convolutions\n    torch.backends.cudnn.benchmark = True\n    \n    # Set memory allocator settings\n    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128,expandable_segments:True'\n    \n    print(\"✓ GPU optimizations enabled:\")\n    print(\"  - TF32 precision\")\n    print(\"  - CuDNN benchmarking\")\n    print(\"  - Memory allocator optimized\")\n\n# Print model info\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"\\nModel Statistics:\")\nprint(f\"  Total parameters: {total_params:,}\")\nprint(f\"  Trainable parameters: {trainable_params:,}\")\nprint(f\"  Model size: ~{total_params * 4 / 1024**2:.2f} MB (float32)\")\nprint(f\"  Model size: ~{total_params * 2 / 1024**2:.2f} MB (float16/mixed precision)\")"
  },
  {
   "cell_type": "markdown",
   "id": "train_funcs_header",
   "metadata": {},
   "source": [
    "## 8. Training and Validation Functions\n",
    "\n",
    "### train_epoch()\n",
    "Trains the model for one complete epoch with mixed precision:\n",
    "1. Set model to training mode\n",
    "2. For each batch:\n",
    "   - Forward pass with autocast (float16)\n",
    "   - Calculate L1 loss\n",
    "   - Backward pass with gradient scaling\n",
    "   - Update weights\n",
    "3. Return average loss\n",
    "\n",
    "### validate()\n",
    "Evaluates model on validation set:\n",
    "1. Set model to eval mode\n",
    "2. Disable gradients for faster inference\n",
    "3. Calculate average loss\n",
    "4. Return validation loss\n",
    "\n",
    "**Memory Management:**\n",
    "- Clears GPU cache between epochs\n",
    "- Uses `zero_grad(set_to_none=True)` for efficient memory cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_funcs",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, scaler, device):\n",
    "    \"\"\"Train for one epoch with mixed precision\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc='Training')\n",
    "    for lr_imgs, hr_imgs in pbar:\n",
    "        lr_imgs = lr_imgs.to(device, non_blocking=True)\n",
    "        hr_imgs = hr_imgs.to(device, non_blocking=True)\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        # Mixed precision forward pass\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            outputs = model(lr_imgs)\n",
    "            loss = criterion(outputs, hr_imgs)\n",
    "        \n",
    "        # Scaled backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.6f}'})\n",
    "    \n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate model performance\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for lr_imgs, hr_imgs in val_loader:\n",
    "            lr_imgs = lr_imgs.to(device, non_blocking=True)\n",
    "            hr_imgs = hr_imgs.to(device, non_blocking=True)\n",
    "            \n",
    "            with torch.amp.autocast('cuda'):\n",
    "                outputs = model(lr_imgs)\n",
    "                loss = criterion(outputs, hr_imgs)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "    \n",
    "    return running_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train_loop_header",
   "metadata": {},
   "source": [
    "## 9. Training Loop\n",
    "\n",
    "Main training loop that runs for all epochs.\n",
    "\n",
    "### Process:\n",
    "1. **Clear GPU cache** at start of each epoch\n",
    "2. **Train** on training set\n",
    "3. **Validate** on validation set\n",
    "4. **Record** losses to history\n",
    "5. **Save checkpoints**:\n",
    "   - Best model (lowest validation loss)\n",
    "   - Periodic checkpoints every 10 epochs\n",
    "\n",
    "### What to Monitor:\n",
    "- Training loss should decrease steadily\n",
    "- Validation loss should track training loss\n",
    "- Large gap indicates overfitting\n",
    "- GPU memory usage (printed after each epoch)\n",
    "\n",
    "**Training Time:**\n",
    "- ConvKAN is slower than standard CNN due to spline computations\n",
    "- Expect ~30-60 seconds per epoch (GPU-dependent)\n",
    "- Total training: ~30-50 minutes for 50 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_loop",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': []\n",
    "}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Starting ConvKAN Training\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Clear GPU cache\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, scaler, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Record\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nTrain Loss: {train_loss:.6f}\")\n",
    "    print(f\"Val Loss:   {val_loss:.6f}\")\n",
    "    \n",
    "    # GPU memory stats\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU Memory: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB peak\")\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'train_loss': train_loss,\n",
    "        }, CHECKPOINT_DIR / 'best_model.pth')\n",
    "        print(f\"✓ Saved best model (val_loss: {val_loss:.6f})\")\n",
    "    \n",
    "    # Periodic checkpoint\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "        }, CHECKPOINT_DIR / f'checkpoint_epoch_{epoch+1}.pth')\n",
    "        print(f\"✓ Saved checkpoint: epoch_{epoch+1}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Complete!\")\n",
    "print(f\"Best validation loss: {best_val_loss:.6f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plot_header",
   "metadata": {},
   "source": [
    "## 10. Training Curve Visualization\n",
    "\n",
    "Plot training and validation loss curves over all epochs.\n",
    "\n",
    "**Analysis:**\n",
    "- **Decreasing trend**: Both curves should trend downward\n",
    "- **Convergence**: Curves should flatten near the end\n",
    "- **Overfitting**: Val loss increasing while train loss decreases\n",
    "- **Underfitting**: Both losses remain high\n",
    "\n",
    "The plot is saved for documentation and model comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history['train_loss'], label='Training Loss', marker='o', alpha=0.7)\n",
    "plt.plot(history['val_loss'], label='Validation Loss', marker='s', alpha=0.7)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (L1)')\n",
    "plt.title('ConvKAN Training Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(CHECKPOINT_DIR / 'training_curve.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training curve saved: {CHECKPOINT_DIR / 'training_curve.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary_header",
   "metadata": {},
   "source": [
    "## 11. Training Summary\n",
    "\n",
    "Generate and save a comprehensive training summary including:\n",
    "- Model architecture details\n",
    "- Hyperparameters\n",
    "- Final performance metrics\n",
    "- File paths to saved models\n",
    "\n",
    "This summary is saved as a text file for experiment tracking and comparison with other models (e.g., U-Net)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model for final summary\n",
    "checkpoint = torch.load(CHECKPOINT_DIR / 'best_model.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "summary = f\"\"\"\n",
    "ConvKAN Super-Resolution Training Summary\n",
    "{'='*60}\n",
    "\n",
    "Model Architecture:\n",
    "  Type: ConvKAN with Residual Blocks\n",
    "  Base Filters: {BASE_FILTERS}\n",
    "  Residual Blocks: {N_RES_BLOCKS}\n",
    "  Total Parameters: {total_params:,}\n",
    "\n",
    "Training Configuration:\n",
    "  Epochs: {NUM_EPOCHS}\n",
    "  Batch Size: {BATCH_SIZE}\n",
    "  Learning Rate: {LEARNING_RATE}\n",
    "  Loss Function: L1 Loss (MAE)\n",
    "  Optimizer: AdamW with weight decay\n",
    "  Mixed Precision: Enabled\n",
    "\n",
    "Dataset:\n",
    "  Total Images: {len(full_dataset)}\n",
    "  Training: {train_size} images\n",
    "  Validation: {val_size} images\n",
    "  Input: 64×64 LR → bicubic 256×256\n",
    "  Output: 256×256 HR\n",
    "\n",
    "Final Results:\n",
    "  Best Validation Loss: {best_val_loss:.6f}\n",
    "  Best Epoch: {checkpoint['epoch'] + 1}\n",
    "  Final Train Loss: {checkpoint['train_loss']:.6f}\n",
    "\n",
    "Saved Files:\n",
    "  Best Model: {CHECKPOINT_DIR / 'best_model.pth'}\n",
    "  Training Curve: {CHECKPOINT_DIR / 'training_curve.png'}\n",
    "  \n",
    "Notes:\n",
    "  - Model processes bicubic-upsampled images (same as UNet)\n",
    "  - No PixelShuffle - constant 256×256 resolution\n",
    "  - Memory-optimized for GPU training\n",
    "  - Ready for evaluation with evaluate_convkan.ipynb\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "with open(CHECKPOINT_DIR / 'training_summary.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print(f\"\\nSummary saved: {CHECKPOINT_DIR / 'training_summary.txt'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next_steps_header",
   "metadata": {},
   "source": [
    "## 12. Next Steps\n",
    "\n",
    "Training is complete! To evaluate your model:\n",
    "\n",
    "1. **Run the evaluation notebook**: `evaluate_convkan.ipynb`\n",
    "   - Calculates PSNR and SSIM metrics\n",
    "   - Generates visual comparisons (LR vs SR vs HR)\n",
    "   - Side-by-side comparison with U-Net results\n",
    "\n",
    "2. **Compare with U-Net**:\n",
    "   - Check if ConvKAN achieves better PSNR/SSIM\n",
    "   - Analyze visual quality differences\n",
    "   - Consider training time vs performance trade-off\n",
    "\n",
    "3. **Further Improvements** (if needed):\n",
    "   - Increase `BASE_FILTERS` to 32 or 64 (if GPU memory allows)\n",
    "   - Add more residual blocks (`N_RES_BLOCKS = 6-8`)\n",
    "   - Train for more epochs (100+)\n",
    "   - Try learning rate scheduling\n",
    "   - Experiment with perceptual loss instead of L1\n",
    "\n",
    "**Model saved at:** `checkpoints_convkan/best_model.pth`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}