{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Patch-based 4K Super-Resolution Training with Mixed Precision\n",
    "\n",
    "This notebook implements a **patch-based training strategy** for 16× super-resolution (256×256 → 4096×4096).\n",
    "\n",
    "## Training Strategy:\n",
    "- **Train on patches**: Extract small patches from large images\n",
    "- **LR patch size**: 64×64 or 128×128\n",
    "- **HR patch size**: 1024×1024 or 2048×2048 (16× larger)\n",
    "- **Inference**: Sliding window over full image, stitch results\n",
    "\n",
    "## Advantages:\n",
    "- **Very low memory**: Can train on 4GB GPU\n",
    "- **Fast iteration**: Small patches train quickly\n",
    "- **Scalable**: Works for any resolution\n",
    "- **Mixed precision**: Further reduces memory\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "device_config",
   "metadata": {},
   "source": [
    "## 2. Device Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "device",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cuda\n",
      "GPU: Quadro RTX 6000\n",
      "显存: 22.15 GB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'使用设备: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'显存: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patch_dataset",
   "metadata": {},
   "source": [
    "## 3. Patch-based Dataset\n",
    "\n",
    "### Key Features:\n",
    "- Randomly extracts patches from 4K images during training\n",
    "- LR patch: 64×64, HR patch: 1024×1024 (16× scale)\n",
    "- Each epoch sees different random patches\n",
    "- Data augmentation: random flip, rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchSRDataset(Dataset):\n",
    "    \"\"\"基于分块的超分辨率数据集\n",
    "    \n",
    "    从4K图像中随机裁剪patch进行训练\n",
    "    \n",
    "    Args:\n",
    "        hr_dir: 4096×4096 HR图像目录\n",
    "        lr_patch_size: LR patch大小（如64或128）\n",
    "        scale_factor: 放大倍数（16）\n",
    "        patches_per_image: 每张图像提取的patch数量\n",
    "        augment: 是否使用数据增强\n",
    "    \"\"\"\n",
    "    def __init__(self, hr_dir, lr_patch_size=64, scale_factor=16, \n",
    "                 patches_per_image=10, augment=True):\n",
    "        self.hr_dir = Path(hr_dir)\n",
    "        self.lr_patch_size = lr_patch_size\n",
    "        self.hr_patch_size = lr_patch_size * scale_factor\n",
    "        self.scale_factor = scale_factor\n",
    "        self.patches_per_image = patches_per_image\n",
    "        self.augment = augment\n",
    "        \n",
    "        # 获取所有HR图片\n",
    "        self.hr_images = sorted(list(self.hr_dir.glob('*.png')))\n",
    "        \n",
    "        print(f\"数据集信息:\")\n",
    "        print(f\"  图像数量: {len(self.hr_images)}\")\n",
    "        print(f\"  LR patch: {lr_patch_size}×{lr_patch_size}\")\n",
    "        print(f\"  HR patch: {self.hr_patch_size}×{self.hr_patch_size}\")\n",
    "        print(f\"  每张图像patch数: {patches_per_image}\")\n",
    "        print(f\"  总patch数/epoch: {len(self.hr_images) * patches_per_image}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.hr_images) * self.patches_per_image\n",
    "    \n",
    "    def augment_patch(self, lr_patch, hr_patch):\n",
    "        \"\"\"数据增强：随机翻转和旋转\"\"\"\n",
    "        # 随机水平翻转\n",
    "        if random.random() > 0.5:\n",
    "            lr_patch = np.fliplr(lr_patch)\n",
    "            hr_patch = np.fliplr(hr_patch)\n",
    "        \n",
    "        # 随机垂直翻转\n",
    "        if random.random() > 0.5:\n",
    "            lr_patch = np.flipud(lr_patch)\n",
    "            hr_patch = np.flipud(hr_patch)\n",
    "        \n",
    "        # 随机旋转90度\n",
    "        k = random.randint(0, 3)\n",
    "        if k > 0:\n",
    "            lr_patch = np.rot90(lr_patch, k)\n",
    "            hr_patch = np.rot90(hr_patch, k)\n",
    "        \n",
    "        return lr_patch.copy(), hr_patch.copy()\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 确定是哪张图像\n",
    "        img_idx = idx // self.patches_per_image\n",
    "        \n",
    "        # 读取HR图像（4096×4096）\n",
    "        hr_img = cv2.imread(str(self.hr_images[img_idx]))\n",
    "        hr_img = cv2.cvtColor(hr_img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # 随机裁剪HR patch\n",
    "        h, w = hr_img.shape[:2]\n",
    "        \n",
    "        # 确保patch在图像范围内\n",
    "        max_y = h - self.hr_patch_size\n",
    "        max_x = w - self.hr_patch_size\n",
    "        \n",
    "        if max_y <= 0 or max_x <= 0:\n",
    "            # 如果图像太小，直接resize\n",
    "            hr_patch = cv2.resize(hr_img, (self.hr_patch_size, self.hr_patch_size))\n",
    "            y, x = 0, 0\n",
    "        else:\n",
    "            y = random.randint(0, max_y)\n",
    "            x = random.randint(0, max_x)\n",
    "            hr_patch = hr_img[y:y+self.hr_patch_size, x:x+self.hr_patch_size]\n",
    "        \n",
    "        # 生成对应的LR patch（下采样）\n",
    "        lr_patch = cv2.resize(hr_patch, (self.lr_patch_size, self.lr_patch_size), \n",
    "                             interpolation=cv2.INTER_CUBIC)\n",
    "        \n",
    "        # 数据增强\n",
    "        if self.augment:\n",
    "            lr_patch, hr_patch = self.augment_patch(lr_patch, hr_patch)\n",
    "        \n",
    "        # 转换为tensor，归一化到[0, 1]\n",
    "        lr_tensor = torch.from_numpy(lr_patch.transpose(2, 0, 1)).float() / 255.0\n",
    "        hr_tensor = torch.from_numpy(hr_patch.transpose(2, 0, 1)).float() / 255.0\n",
    "        \n",
    "        return lr_tensor, hr_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_arch",
   "metadata": {},
   "source": [
    "## 4. Efficient U-Net Model for 16× SR\n",
    "\n",
    "### Architecture:\n",
    "- Designed for 16× super-resolution (64×64 → 1024×1024)\n",
    "- Uses progressive upsampling (4 stages of 2× each)\n",
    "- Lightweight with ~10M parameters\n",
    "- BatchNorm for training stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "model",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"双卷积块\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"下采样块\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"上采样块\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, skip_channels=None):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "        \n",
    "        if skip_channels is not None:\n",
    "            self.conv = DoubleConv(in_channels // 2 + skip_channels, out_channels)\n",
    "        else:\n",
    "            self.conv = DoubleConv(in_channels // 2, out_channels)\n",
    "        \n",
    "        self.has_skip = skip_channels is not None\n",
    "    \n",
    "    def forward(self, x1, x2=None):\n",
    "        x1 = self.up(x1)\n",
    "        if self.has_skip and x2 is not None:\n",
    "            x = torch.cat([x2, x1], dim=1)\n",
    "        else:\n",
    "            x = x1\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class UNetPatch16x(nn.Module):\n",
    "    \"\"\"基于Patch的16×超分辨率U-Net\n",
    "    \n",
    "    输入: 64×64 (或128×128)\n",
    "    输出: 1024×1024 (或2048×2048)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_channels=3, n_classes=3, base_channels=48):\n",
    "        super(UNetPatch16x, self).__init__()\n",
    "        \n",
    "        # 编码器（64 -> 4）\n",
    "        self.inc = DoubleConv(n_channels, base_channels)           # 64×64\n",
    "        self.down1 = Down(base_channels, base_channels * 2)        # 32×32\n",
    "        self.down2 = Down(base_channels * 2, base_channels * 4)    # 16×16\n",
    "        self.down3 = Down(base_channels * 4, base_channels * 8)    # 8×8\n",
    "        self.down4 = Down(base_channels * 8, base_channels * 16)   # 4×4 (bottleneck)\n",
    "        \n",
    "        # 解码器（4 -> 64，带skip连接）\n",
    "        self.up1 = Up(base_channels * 16, base_channels * 8, skip_channels=base_channels * 8)   # 8×8\n",
    "        self.up2 = Up(base_channels * 8, base_channels * 4, skip_channels=base_channels * 4)    # 16×16\n",
    "        self.up3 = Up(base_channels * 4, base_channels * 2, skip_channels=base_channels * 2)    # 32×32\n",
    "        self.up4 = Up(base_channels * 2, base_channels, skip_channels=base_channels)            # 64×64\n",
    "        \n",
    "        # 额外的上采样层（64 -> 1024，16×放大）\n",
    "        # 64 -> 128 -> 256 -> 512 -> 1024\n",
    "        self.up5 = Up(base_channels, base_channels, skip_channels=None)      # 128×128\n",
    "        self.up6 = Up(base_channels, base_channels, skip_channels=None)      # 256×256\n",
    "        self.up7 = Up(base_channels, base_channels, skip_channels=None)      # 512×512\n",
    "        self.up8 = Up(base_channels, base_channels, skip_channels=None)      # 1024×1024\n",
    "        \n",
    "        # 输出层\n",
    "        self.outc = nn.Conv2d(base_channels, n_classes, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 编码\n",
    "        x1 = self.inc(x)       # 48, 64×64\n",
    "        x2 = self.down1(x1)    # 96, 32×32\n",
    "        x3 = self.down2(x2)    # 192, 16×16\n",
    "        x4 = self.down3(x3)    # 384, 8×8\n",
    "        x5 = self.down4(x4)    # 768, 4×4\n",
    "        \n",
    "        # 解码（带skip连接）\n",
    "        x = self.up1(x5, x4)   # 384, 8×8\n",
    "        x = self.up2(x, x3)    # 192, 16×16\n",
    "        x = self.up3(x, x2)    # 96, 32×32\n",
    "        x = self.up4(x, x1)    # 48, 64×64\n",
    "        \n",
    "        # 额外上采样到16×\n",
    "        x = self.up5(x)        # 48, 128×128\n",
    "        x = self.up6(x)        # 48, 256×256\n",
    "        x = self.up7(x)        # 48, 512×512\n",
    "        x = self.up8(x)        # 48, 1024×1024\n",
    "        \n",
    "        # 输出\n",
    "        return self.outc(x)    # 3, 1024×1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config",
   "metadata": {},
   "source": [
    "## 5. Training Configuration\n",
    "\n",
    "### Memory-Efficient Settings:\n",
    "- **LR patch**: 64×64 (very small)\n",
    "- **HR patch**: 1024×1024 (16× scale)\n",
    "- **Batch size**: 4-8 (fits easily in GPU)\n",
    "- **Mixed precision**: Enabled\n",
    "- **Patches per image**: 10 (more variety per epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfd0cab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练配置:\n",
      "  LR patch大小: 64×64\n",
      "  HR patch大小: 1024×1024\n",
      "  放大倍数: 16×\n",
      "  Batch size: 4\n",
      "  训练轮数: 50\n",
      "  每张图像patches: 10\n",
      "  混合精度: 启用\n",
      "  模型基础通道数: 48\n"
     ]
    }
   ],
   "source": [
    "# 训练配置\n",
    "LR_PATCH_SIZE = 32          # LR patch大小（64×64或128×128）\n",
    "SCALE_FACTOR = 16           # 放大倍数\n",
    "HR_PATCH_SIZE = LR_PATCH_SIZE * SCALE_FACTOR  # 1024×1024\n",
    "\n",
    "BATCH_SIZE = 1              # batch size（可以设置更大，如8或16）\n",
    "LEARNING_RATE = 2e-4        # 学习率\n",
    "NUM_EPOCHS = 50             # 训练轮数\n",
    "PATCHES_PER_IMAGE = 10      # 每张图像提取的patch数\n",
    "\n",
    "BASE_CHANNELS = 32          # 模型基础通道数（可调整：32/48/64）\n",
    "USE_MIXED_PRECISION = True  # 混合精度训练\n",
    "TRAIN_SPLIT = 0.9           # 训练集比例\n",
    "\n",
    "# 数据路径\n",
    "HR_DIR = './dataset_4k/high_resolution'\n",
    "CHECKPOINT_DIR = Path('./checkpoints_patch')\n",
    "CHECKPOINT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"训练配置:\")\n",
    "print(f\"  LR patch大小: {LR_PATCH_SIZE}×{LR_PATCH_SIZE}\")\n",
    "print(f\"  HR patch大小: {HR_PATCH_SIZE}×{HR_PATCH_SIZE}\")\n",
    "print(f\"  放大倍数: {SCALE_FACTOR}×\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  训练轮数: {NUM_EPOCHS}\")\n",
    "print(f\"  每张图像patches: {PATCHES_PER_IMAGE}\")\n",
    "print(f\"  混合精度: {'启用' if USE_MIXED_PRECISION else '禁用'}\")\n",
    "print(f\"  模型基础通道数: {BASE_CHANNELS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_loading",
   "metadata": {},
   "source": [
    "## 6. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "data_loader",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集信息:\n",
      "  图像数量: 300\n",
      "  LR patch: 64×64\n",
      "  HR patch: 1024×1024\n",
      "  每张图像patch数: 10\n",
      "  总patch数/epoch: 3000\n",
      "\n",
      "训练集patches: 2700\n",
      "验证集patches: 300\n",
      "训练批次数: 675\n"
     ]
    }
   ],
   "source": [
    "# 创建数据集\n",
    "full_dataset = PatchSRDataset(\n",
    "    hr_dir=HR_DIR,\n",
    "    lr_patch_size=LR_PATCH_SIZE,\n",
    "    scale_factor=SCALE_FACTOR,\n",
    "    patches_per_image=PATCHES_PER_IMAGE,\n",
    "    augment=True\n",
    ")\n",
    "\n",
    "# 划分训练集和验证集\n",
    "train_size = int(TRAIN_SPLIT * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    full_dataset, [train_size, val_size]\n",
    ")\n",
    "\n",
    "# 数据加载器\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2,  # 可以使用多进程加载\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\n训练集patches: {train_size}\")\n",
    "print(f\"验证集patches: {val_size}\")\n",
    "print(f\"训练批次数: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize_patches",
   "metadata": {},
   "source": [
    "## 7. Visualize Sample Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "visualize",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x600 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本已保存至: checkpoints_patch/patch_samples.png\n"
     ]
    }
   ],
   "source": [
    "# 可视化一些patch样本\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "\n",
    "for i in range(5):\n",
    "    lr_patch, hr_patch = full_dataset[i]\n",
    "    \n",
    "    # 转换为numpy\n",
    "    lr_np = lr_patch.numpy().transpose(1, 2, 0)\n",
    "    hr_np = hr_patch.numpy().transpose(1, 2, 0)\n",
    "    \n",
    "    # 显示LR patch\n",
    "    axes[0, i].imshow(lr_np)\n",
    "    axes[0, i].set_title(f'LR {LR_PATCH_SIZE}×{LR_PATCH_SIZE}')\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # 显示HR patch（下采样显示）\n",
    "    hr_display = cv2.resize(hr_np, (256, 256), interpolation=cv2.INTER_AREA)\n",
    "    axes[1, i].imshow(hr_display)\n",
    "    axes[1, i].set_title(f'HR {HR_PATCH_SIZE}×{HR_PATCH_SIZE}')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(CHECKPOINT_DIR / 'patch_samples.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"样本已保存至: {CHECKPOINT_DIR / 'patch_samples.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_init",
   "metadata": {},
   "source": [
    "## 8. Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "init_model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "模型参数总数: 17,610,003\n",
      "优化器: Adam\n",
      "学习率: 0.0002\n",
      "损失函数: L1 Loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2200622/3732452908.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler() if USE_MIXED_PRECISION else None\n"
     ]
    }
   ],
   "source": [
    "# 初始化模型\n",
    "model = UNetPatch16x(n_channels=3, n_classes=3, base_channels=BASE_CHANNELS).to(device)\n",
    "\n",
    "# 统计参数\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\n模型参数总数: {total_params:,}\")\n",
    "\n",
    "# 损失函数和优化器\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.5)\n",
    "\n",
    "# 混合精度scaler\n",
    "scaler = GradScaler() if USE_MIXED_PRECISION else None\n",
    "\n",
    "print(f\"优化器: Adam\")\n",
    "print(f\"学习率: {LEARNING_RATE}\")\n",
    "print(f\"损失函数: L1 Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train_functions",
   "metadata": {},
   "source": [
    "## 9. Training and Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "train_val_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, scaler, device, use_amp=True):\n",
    "    \"\"\"训练一个epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc='训练')\n",
    "    for lr_patches, hr_patches in pbar:\n",
    "        lr_patches = lr_patches.to(device)\n",
    "        hr_patches = hr_patches.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 混合精度训练\n",
    "        if use_amp and scaler is not None:\n",
    "            with autocast():\n",
    "                outputs = model(lr_patches)\n",
    "                loss = criterion(outputs, hr_patches)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            outputs = model(lr_patches)\n",
    "            loss = criterion(outputs, hr_patches)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, device, use_amp=True):\n",
    "    \"\"\"验证\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for lr_patches, hr_patches in val_loader:\n",
    "            lr_patches = lr_patches.to(device)\n",
    "            hr_patches = hr_patches.to(device)\n",
    "            \n",
    "            if use_amp:\n",
    "                with autocast():\n",
    "                    outputs = model(lr_patches)\n",
    "                    loss = criterion(outputs, hr_patches)\n",
    "            else:\n",
    "                outputs = model(lr_patches)\n",
    "                loss = criterion(outputs, hr_patches)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "    \n",
    "    return running_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training_loop",
   "metadata": {},
   "source": [
    "## 10. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "train",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "开始训练...\n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练:   0%|                                             | 0/675 [00:00<?, ?it/s]/tmp/ipykernel_2200622/2837316200.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "训练:   0%|                                             | 0/675 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 22.15 GiB of which 65.88 MiB is free. Process 2189641 has 21.24 GiB memory in use. Including non-PyTorch memory, this process has 856.00 MiB memory in use. Of the allocated memory 558.01 MiB is allocated by PyTorch, and 105.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m50\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# 训练\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mUSE_MIXED_PRECISION\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# 验证\u001b[39;00m\n\u001b[32m     15\u001b[39m val_loss = validate(model, val_loader, criterion, device, USE_MIXED_PRECISION)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, train_loader, criterion, optimizer, scaler, device, use_amp)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_amp \u001b[38;5;129;01mand\u001b[39;00m scaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m autocast():\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m         outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr_patches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m         loss = criterion(outputs, hr_patches)\n\u001b[32m     19\u001b[39m     scaler.scale(loss).backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/classification/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/classification/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 102\u001b[39m, in \u001b[36mUNetPatch16x.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    100\u001b[39m x = \u001b[38;5;28mself\u001b[39m.up5(x)        \u001b[38;5;66;03m# 48, 128×128\u001b[39;00m\n\u001b[32m    101\u001b[39m x = \u001b[38;5;28mself\u001b[39m.up6(x)        \u001b[38;5;66;03m# 48, 256×256\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mup7\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m        \u001b[38;5;66;03m# 48, 512×512\u001b[39;00m\n\u001b[32m    103\u001b[39m x = \u001b[38;5;28mself\u001b[39m.up8(x)        \u001b[38;5;66;03m# 48, 1024×1024\u001b[39;00m\n\u001b[32m    105\u001b[39m \u001b[38;5;66;03m# 输出\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/classification/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/classification/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36mUp.forward\u001b[39m\u001b[34m(self, x1, x2)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     49\u001b[39m     x = x1\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/classification/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/classification/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mDoubleConv.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdouble_conv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/classification/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/classification/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/classification/lib/python3.13/site-packages/torch/nn/modules/container.py:244\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/classification/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/classification/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/classification/lib/python3.13/site-packages/torch/nn/modules/conv.py:548\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/classification/lib/python3.13/site-packages/torch/nn/modules/conv.py:543\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    533\u001b[39m         F.pad(\n\u001b[32m    534\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    541\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    542\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 22.15 GiB of which 65.88 MiB is free. Process 2189641 has 21.24 GiB memory in use. Including non-PyTorch memory, this process has 856.00 MiB memory in use. Of the allocated memory 558.01 MiB is allocated by PyTorch, and 105.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# 训练历史\n",
    "history = {'train_loss': [], 'val_loss': []}\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "print(\"\\n开始训练...\\n\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # 训练\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, scaler, device, USE_MIXED_PRECISION)\n",
    "    \n",
    "    # 验证\n",
    "    val_loss = validate(model, val_loader, criterion, device, USE_MIXED_PRECISION)\n",
    "    \n",
    "    # 记录\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    \n",
    "    # 更新学习率\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"\\n训练损失: {train_loss:.6f}\")\n",
    "    print(f\"验证损失: {val_loss:.6f}\")\n",
    "    print(f\"学习率: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "    \n",
    "    # 保存最佳模型\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'config': {\n",
    "                'lr_patch_size': LR_PATCH_SIZE,\n",
    "                'scale_factor': SCALE_FACTOR,\n",
    "                'base_channels': BASE_CHANNELS\n",
    "            }\n",
    "        }, CHECKPOINT_DIR / 'best_model.pth')\n",
    "        print(f\"✓ 保存最佳模型 (验证损失: {val_loss:.6f})\")\n",
    "    \n",
    "    # 定期保存\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "        }, CHECKPOINT_DIR / f'checkpoint_epoch_{epoch+1}.pth')\n",
    "        print(f\"✓ 保存检查点: epoch_{epoch+1}\")\n",
    "\n",
    "print(\"\\n训练完成！\")\n",
    "print(f\"最佳验证损失: {best_val_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plot_curve",
   "metadata": {},
   "source": [
    "## 11. Training Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history['train_loss'], label='Training Loss', marker='o')\n",
    "plt.plot(history['val_loss'], label='Validation Loss', marker='s')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (L1)')\n",
    "plt.title('Patch-based Training Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(CHECKPOINT_DIR / 'training_curve.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"训练曲线已保存: {CHECKPOINT_DIR / 'training_curve.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inference",
   "metadata": {},
   "source": [
    "## 12. Inference with Sliding Window\n",
    "\n",
    "### Sliding Window Strategy:\n",
    "- Divide 256×256 LR image into overlapping 64×64 patches\n",
    "- Process each patch → 1024×1024 HR patch\n",
    "- Stitch all HR patches → 4096×4096 final image\n",
    "- Use overlap and blending to avoid seams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inference_func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_inference(model, lr_img_256, patch_size=64, overlap=16, device='cuda'):\n",
    "    \"\"\"\n",
    "    使用滑动窗口对256×256图像进行16×超分辨率\n",
    "    \n",
    "    Args:\n",
    "        model: 训练好的模型\n",
    "        lr_img_256: 256×256的LR图像 (torch.Tensor, shape: 1,3,256,256)\n",
    "        patch_size: patch大小（64）\n",
    "        overlap: patch之间的重叠（16，用于平滑拼接）\n",
    "        device: 设备\n",
    "    \n",
    "    Returns:\n",
    "        sr_img_4k: 4096×4096的SR图像 (torch.Tensor)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    _, _, h, w = lr_img_256.shape\n",
    "    stride = patch_size - overlap\n",
    "    \n",
    "    # 计算需要多少个patch\n",
    "    num_patches_h = (h - overlap) // stride\n",
    "    num_patches_w = (w - overlap) // stride\n",
    "    \n",
    "    # 输出图像大小\n",
    "    out_h = num_patches_h * patch_size * 16\n",
    "    out_w = num_patches_w * patch_size * 16\n",
    "    \n",
    "    # 创建输出canvas\n",
    "    sr_img = torch.zeros(1, 3, out_h, out_w).to(device)\n",
    "    weight_map = torch.zeros(1, 1, out_h, out_w).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with autocast():\n",
    "            for i in range(num_patches_h):\n",
    "                for j in range(num_patches_w):\n",
    "                    # 提取LR patch\n",
    "                    y = i * stride\n",
    "                    x = j * stride\n",
    "                    lr_patch = lr_img_256[:, :, y:y+patch_size, x:x+patch_size]\n",
    "                    \n",
    "                    # 超分辨率\n",
    "                    sr_patch = model(lr_patch.to(device))\n",
    "                    \n",
    "                    # 放置到输出图像\n",
    "                    out_y = i * stride * 16\n",
    "                    out_x = j * stride * 16\n",
    "                    sr_img[:, :, out_y:out_y+patch_size*16, out_x:out_x+patch_size*16] += sr_patch\n",
    "                    weight_map[:, :, out_y:out_y+patch_size*16, out_x:out_x+patch_size*16] += 1\n",
    "    \n",
    "    # 平均重叠区域\n",
    "    sr_img = sr_img / weight_map\n",
    "    \n",
    "    return sr_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test_inference",
   "metadata": {},
   "source": [
    "## 13. Test Inference on Full Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载最佳模型\n",
    "checkpoint = torch.load(CHECKPOINT_DIR / 'best_model.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"已加载最佳模型 (Epoch {checkpoint['epoch']+1}, 验证损失: {checkpoint['val_loss']:.6f})\")\n",
    "\n",
    "# 读取测试图像\n",
    "test_hr_path = list(Path(HR_DIR).glob('*.png'))[0]\n",
    "test_hr_4k = cv2.imread(str(test_hr_path))\n",
    "test_hr_4k = cv2.cvtColor(test_hr_4k, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# 生成256×256的LR图像\n",
    "test_lr_256 = cv2.resize(test_hr_4k, (256, 256), interpolation=cv2.INTER_CUBIC)\n",
    "lr_tensor = torch.from_numpy(test_lr_256.transpose(2, 0, 1)).float() / 255.0\n",
    "lr_tensor = lr_tensor.unsqueeze(0)\n",
    "\n",
    "print(f\"\\n输入图像: {lr_tensor.shape}\")\n",
    "print(\"开始滑动窗口推理...\")\n",
    "\n",
    "# 滑动窗口推理\n",
    "sr_tensor = sliding_window_inference(model, lr_tensor, patch_size=LR_PATCH_SIZE, \n",
    "                                     overlap=16, device=device)\n",
    "\n",
    "print(f\"输出图像: {sr_tensor.shape}\")\n",
    "\n",
    "# 可视化结果\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# LR输入\n",
    "axes[0].imshow(test_lr_256)\n",
    "axes[0].set_title('Input (256×256)')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# SR输出（下采样显示）\n",
    "sr_np = sr_tensor.squeeze(0).cpu().numpy().transpose(1, 2, 0)\n",
    "sr_np = np.clip(sr_np, 0, 1)\n",
    "sr_display = cv2.resize(sr_np, (512, 512), interpolation=cv2.INTER_AREA)\n",
    "axes[1].imshow(sr_display)\n",
    "axes[1].set_title(f'SR Output ({sr_tensor.shape[2]}×{sr_tensor.shape[3]})')\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Ground Truth（下采样显示）\n",
    "hr_np = test_hr_4k / 255.0\n",
    "hr_display = cv2.resize(hr_np, (512, 512), interpolation=cv2.INTER_AREA)\n",
    "axes[2].imshow(hr_display)\n",
    "axes[2].set_title('Ground Truth (4096×4096)')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(CHECKPOINT_DIR / 'inference_result.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n推理结果已保存: {CHECKPOINT_DIR / 'inference_result.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval",
   "metadata": {},
   "source": [
    "## 14. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "def evaluate_full_images(model, hr_dir, num_samples=5, patch_size=64, device='cuda'):\n",
    "    \"\"\"在完整图像上评估模型\"\"\"\n",
    "    hr_images = sorted(list(Path(hr_dir).glob('*.png')))[:num_samples]\n",
    "    \n",
    "    psnr_scores = []\n",
    "    ssim_scores = []\n",
    "    \n",
    "    for hr_path in tqdm(hr_images, desc='评估'):\n",
    "        # 读取HR图像\n",
    "        hr_4k = cv2.imread(str(hr_path))\n",
    "        hr_4k = cv2.cvtColor(hr_4k, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # 生成LR图像\n",
    "        lr_256 = cv2.resize(hr_4k, (256, 256), interpolation=cv2.INTER_CUBIC)\n",
    "        lr_tensor = torch.from_numpy(lr_256.transpose(2, 0, 1)).float() / 255.0\n",
    "        lr_tensor = lr_tensor.unsqueeze(0)\n",
    "        \n",
    "        # 推理\n",
    "        sr_tensor = sliding_window_inference(model, lr_tensor, patch_size=patch_size, \n",
    "                                            overlap=16, device=device)\n",
    "        \n",
    "        # 转换为numpy\n",
    "        sr_np = sr_tensor.squeeze(0).cpu().numpy().transpose(1, 2, 0)\n",
    "        sr_np = np.clip(sr_np, 0, 1)\n",
    "        hr_np = hr_4k / 255.0\n",
    "        \n",
    "        # 裁剪到相同大小（如果有差异）\n",
    "        min_h = min(sr_np.shape[0], hr_np.shape[0])\n",
    "        min_w = min(sr_np.shape[1], hr_np.shape[1])\n",
    "        sr_np = sr_np[:min_h, :min_w]\n",
    "        hr_np = hr_np[:min_h, :min_w]\n",
    "        \n",
    "        # 下采样计算指标\n",
    "        sr_small = cv2.resize(sr_np, (1024, 1024), interpolation=cv2.INTER_AREA)\n",
    "        hr_small = cv2.resize(hr_np, (1024, 1024), interpolation=cv2.INTER_AREA)\n",
    "        \n",
    "        # 计算指标\n",
    "        psnr_score = psnr(hr_small, sr_small, data_range=1.0)\n",
    "        ssim_score = ssim(hr_small, sr_small, data_range=1.0, channel_axis=2)\n",
    "        \n",
    "        psnr_scores.append(psnr_score)\n",
    "        ssim_scores.append(ssim_score)\n",
    "    \n",
    "    print(f\"\\n=== 评估结果 ===\")\n",
    "    print(f\"平均PSNR: {np.mean(psnr_scores):.2f} dB\")\n",
    "    print(f\"平均SSIM: {np.mean(ssim_scores):.4f}\")\n",
    "    \n",
    "    return np.mean(psnr_scores), np.mean(ssim_scores)\n",
    "\n",
    "# 运行评估\n",
    "avg_psnr, avg_ssim = evaluate_full_images(model, HR_DIR, num_samples=5, \n",
    "                                          patch_size=LR_PATCH_SIZE, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## 15. Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = f\"\"\"\n",
    "基于Patch的4K超分辨率训练总结\n",
    "{'='*60}\n",
    "\n",
    "训练策略: Patch-based Training\n",
    "混合精度: {'启用' if USE_MIXED_PRECISION else '禁用'}\n",
    "\n",
    "数据配置:\n",
    "  LR patch大小: {LR_PATCH_SIZE}×{LR_PATCH_SIZE}\n",
    "  HR patch大小: {HR_PATCH_SIZE}×{HR_PATCH_SIZE}\n",
    "  缩放倍数: {SCALE_FACTOR}×\n",
    "  每张图像patches: {PATCHES_PER_IMAGE}\n",
    "\n",
    "模型配置:\n",
    "  架构: UNetPatch16x\n",
    "  基础通道数: {BASE_CHANNELS}\n",
    "  参数量: {total_params:,}\n",
    "\n",
    "训练配置:\n",
    "  Batch size: {BATCH_SIZE}\n",
    "  训练轮数: {NUM_EPOCHS}\n",
    "  初始学习率: {LEARNING_RATE}\n",
    "  优化器: Adam\n",
    "  损失函数: L1 Loss\n",
    "\n",
    "训练结果:\n",
    "  最佳验证损失: {best_val_loss:.6f}\n",
    "  平均PSNR: {avg_psnr:.2f} dB\n",
    "  平均SSIM: {avg_ssim:.4f}\n",
    "\n",
    "模型保存位置: {CHECKPOINT_DIR / 'best_model.pth'}\n",
    "\n",
    "推理方法:\n",
    "  使用滑动窗口（overlap=16）拼接完整4K图像\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "with open(CHECKPOINT_DIR / 'training_summary.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print(f\"\\n总结已保存至: {CHECKPOINT_DIR / 'training_summary.txt'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (classification)",
   "language": "python",
   "name": "classification"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
