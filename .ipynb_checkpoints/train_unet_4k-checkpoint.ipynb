{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "rl9db9fxjug",
   "metadata": {},
   "source": [
    "# U-Net Based 4K Image Super-Resolution Training\n",
    "\n",
    "This notebook implements a U-Net architecture for 4K image super-resolution tasks. The model learns to transform 256×256 low-resolution images into 4096×4096 high-resolution outputs.\n",
    "\n",
    "## Project Overview\n",
    "- **Task**: 4K Image Super-Resolution (SR)\n",
    "- **Architecture**: Deep U-Net with increased depth and channel numbers\n",
    "- **Input**: Low-resolution images (256×256)\n",
    "- **Output**: High-resolution images (4096×4096)\n",
    "- **Scale Factor**: 16×\n",
    "- **Loss Function**: L1 Loss (Mean Absolute Error)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0577d9-928f-4ecf-bba7-231003a647ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k6mb5czjk8l",
   "metadata": {},
   "source": [
    "## 2. Device Configuration\n",
    "\n",
    "Check if CUDA GPU is available for accelerated training. The model will automatically use GPU if available, otherwise fall back to CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0ad23c-140d-4f10-a445-c2f05b3f2196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'使用设备: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vebq33yf9nq",
   "metadata": {},
   "source": [
    "## 3. Dataset Class\n",
    "\n",
    "### SRDataset4K: 4K Super-Resolution Dataset\n",
    "\n",
    "This custom PyTorch Dataset class handles loading paired high-resolution (HR) and low-resolution (LR) images.\n",
    "\n",
    "**Key Features:**\n",
    "- Loads paired HR-LR images from separate directories\n",
    "- HR images are at 4096×4096 resolution\n",
    "- LR images are at 256×256 resolution\n",
    "- Applies transformations (e.g., converting to tensors)\n",
    "- Ensures HR and LR image counts match\n",
    "\n",
    "**Process Flow:**\n",
    "1. Load LR image (256×256)\n",
    "2. Load corresponding HR image (4096×4096)\n",
    "3. Convert both to tensors\n",
    "4. Return (LR, HR) pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecbe2e1-67c5-4b17-b6df-0340666eb992",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRDataset4K(Dataset):\n",
    "    \"\"\"4K超分辨率数据集\"\"\"\n",
    "    def __init__(self, hr_dir, lr_dir, transform=None):\n",
    "        self.hr_dir = Path(hr_dir)\n",
    "        self.lr_dir = Path(lr_dir)\n",
    "        self.transform = transform\n",
    "        \n",
    "        # 获取所有图片文件\n",
    "        self.hr_images = sorted(list(self.hr_dir.glob('*.png')))\n",
    "        self.lr_images = sorted(list(self.lr_dir.glob('*.png')))\n",
    "        \n",
    "        assert len(self.hr_images) == len(self.lr_images), \"HR和LR图片数量不匹配\"\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.hr_images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 读取图片\n",
    "        hr_img = Image.open(self.hr_images[idx]).convert('RGB')\n",
    "        lr_img = Image.open(self.lr_images[idx]).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            hr_img = self.transform(hr_img)\n",
    "            lr_img = self.transform(lr_img)\n",
    "        \n",
    "        return lr_img, hr_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3pc9ryjyzs",
   "metadata": {},
   "source": [
    "## 4. U-Net Model Architecture\n",
    "\n",
    "### Overview of U-Net for 4K Super-Resolution\n",
    "\n",
    "This implementation uses a **deep U-Net** architecture designed specifically for 16× super-resolution (256×256 → 4096×4096).\n",
    "\n",
    "---\n",
    "\n",
    "### 4.1 DoubleConv Block\n",
    "\n",
    "The basic building block of our U-Net. Each DoubleConv consists of:\n",
    "- **Conv2d** (3×3 kernel, padding=1) → preserves spatial dimensions\n",
    "- **BatchNorm2d** → stabilizes training for larger models\n",
    "- **ReLU** activation (inplace for memory efficiency)\n",
    "- **Conv2d** (3×3 kernel, padding=1)\n",
    "- **BatchNorm2d**\n",
    "- **ReLU** activation\n",
    "\n",
    "---\n",
    "\n",
    "### 4.2 Down Block (Encoder)\n",
    "\n",
    "Downsampling block that reduces spatial dimensions while increasing feature channels:\n",
    "1. **MaxPool2d** (2×2) → reduces spatial size by half\n",
    "2. **DoubleConv** → extracts features at this resolution\n",
    "\n",
    "**Purpose**: Extract hierarchical features from coarse to fine.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.3 Up Block (Decoder)\n",
    "\n",
    "Upsampling block that increases spatial dimensions:\n",
    "1. **ConvTranspose2d** (2×2, stride=2) → doubles spatial size\n",
    "2. **Concatenate** with skip connection from encoder\n",
    "3. **DoubleConv** → refines the concatenated features\n",
    "\n",
    "**Skip Connections**: Combines low-level and high-level features for better detail recovery.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.4 Complete U-Net Architecture for 4K\n",
    "\n",
    "#### Encoder Path (Downsampling):\n",
    "```\n",
    "Input: 3 channels (RGB), 256×256\n",
    "    ↓ [DoubleConv]\n",
    "  64 channels, 256×256\n",
    "    ↓ [Down1: MaxPool + DoubleConv]\n",
    " 128 channels, 128×128\n",
    "    ↓ [Down2: MaxPool + DoubleConv]\n",
    " 256 channels, 64×64\n",
    "    ↓ [Down3: MaxPool + DoubleConv]\n",
    " 512 channels, 32×32\n",
    "    ↓ [Down4: MaxPool + DoubleConv]\n",
    "1024 channels, 16×16 (Bottleneck)\n",
    "```\n",
    "\n",
    "#### Decoder Path (Upsampling with 16× scale):\n",
    "```\n",
    "1024 channels, 16×16\n",
    "    ↓ [Up1: TransConv + Concat(512 from Down3) + DoubleConv]\n",
    " 512 channels, 32×32\n",
    "    ↓ [Up2: TransConv + Concat(256 from Down2) + DoubleConv]\n",
    " 256 channels, 64×64\n",
    "    ↓ [Up3: TransConv + Concat(128 from Down1) + DoubleConv]\n",
    " 128 channels, 128×128\n",
    "    ↓ [Up4: TransConv + Concat(64 from inc) + DoubleConv]\n",
    "  64 channels, 256×256\n",
    "    ↓ [Up5: TransConv + DoubleConv] 16× upsampling\n",
    "  64 channels, 512×512\n",
    "    ↓ [Up6: TransConv + DoubleConv]\n",
    "  64 channels, 1024×1024\n",
    "    ↓ [Up7: TransConv + DoubleConv]\n",
    "  64 channels, 2048×2048\n",
    "    ↓ [Up8: TransConv + DoubleConv]\n",
    "  64 channels, 4096×4096\n",
    "    ↓ [Output Conv 1×1]\n",
    "   3 channels, 4096×4096 (Output RGB image)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Model Optimizations for 4K:\n",
    "1. **Deep architecture**: 4 downsampling + 8 upsampling layers for 16× scale\n",
    "2. **BatchNorm**: Added for training stability with high resolution\n",
    "3. **Transposed convolutions**: Learnable upsampling for better quality\n",
    "4. **Progressive upsampling**: Multiple stages to reach 4096×4096\n",
    "\n",
    "**Total Parameters**: ~55M (deeper model for complex 16× SR task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df5757f-9206-4a7d-b987-d4723ee8b9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"双卷积块 (Conv -> BN -> ReLU) * 2\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"下采样块 MaxPool -> DoubleConv\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"上采样块 ConvTranspose -> DoubleConv\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, skip_connection=True):\n",
    "        super().__init__()\n",
    "        self.skip_connection = skip_connection\n",
    "        # 使用转置卷积进行上采样\n",
    "        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "        \n",
    "        if skip_connection:\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "        else:\n",
    "            self.conv = DoubleConv(in_channels // 2, out_channels)\n",
    "    \n",
    "    def forward(self, x1, x2=None):\n",
    "        x1 = self.up(x1)\n",
    "        # 拼接跳跃连接\n",
    "        if self.skip_connection and x2 is not None:\n",
    "            x = torch.cat([x2, x1], dim=1)\n",
    "        else:\n",
    "            x = x1\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class UNet4K(nn.Module):\n",
    "    \"\"\"深度U-Net用于4K超分辨率 (256x256 -> 4096x4096, 16x放大)\"\"\"\n",
    "    def __init__(self, n_channels=3, n_classes=3):\n",
    "        super(UNet4K, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        # 编码器 (256 -> 16)\n",
    "        self.inc = DoubleConv(n_channels, 64)        # 256x256\n",
    "        self.down1 = Down(64, 128)                   # 128x128\n",
    "        self.down2 = Down(128, 256)                  # 64x64\n",
    "        self.down3 = Down(256, 512)                  # 32x32\n",
    "        self.down4 = Down(512, 1024)                 # 16x16 (bottleneck)\n",
    "        \n",
    "        # 解码器 (16 -> 256 with skip connections)\n",
    "        self.up1 = Up(1024 + 512, 512)               # 32x32\n",
    "        self.up2 = Up(512 + 256, 256)                # 64x64\n",
    "        self.up3 = Up(256 + 128, 128)                # 128x128\n",
    "        self.up4 = Up(128 + 64, 64)                  # 256x256\n",
    "        \n",
    "        # 额外的上采样层 (256 -> 4096, 16x upsampling)\n",
    "        self.up5 = Up(64, 64, skip_connection=False)   # 512x512\n",
    "        self.up6 = Up(64, 64, skip_connection=False)   # 1024x1024\n",
    "        self.up7 = Up(64, 64, skip_connection=False)   # 2048x2048\n",
    "        self.up8 = Up(64, 64, skip_connection=False)   # 4096x4096\n",
    "        \n",
    "        # 输出层\n",
    "        self.outc = nn.Conv2d(64, n_classes, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 编码路径\n",
    "        x1 = self.inc(x)      # 64, 256x256\n",
    "        x2 = self.down1(x1)   # 128, 128x128\n",
    "        x3 = self.down2(x2)   # 256, 64x64\n",
    "        x4 = self.down3(x3)   # 512, 32x32\n",
    "        x5 = self.down4(x4)   # 1024, 16x16\n",
    "        \n",
    "        # 解码路径（带跳跃连接）\n",
    "        x = self.up1(x5, x4)  # 512, 32x32\n",
    "        x = self.up2(x, x3)   # 256, 64x64\n",
    "        x = self.up3(x, x2)   # 128, 128x128\n",
    "        x = self.up4(x, x1)   # 64, 256x256\n",
    "        \n",
    "        # 额外的上采样到4K\n",
    "        x = self.up5(x)       # 64, 512x512\n",
    "        x = self.up6(x)       # 64, 1024x1024\n",
    "        x = self.up7(x)       # 64, 2048x2048\n",
    "        x = self.up8(x)       # 64, 4096x4096\n",
    "        \n",
    "        # 输出\n",
    "        return self.outc(x)   # 3, 4096x4096"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dtv0owwgzi",
   "metadata": {},
   "source": [
    "## 5. Training Configuration\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "These parameters are tuned for **4K super-resolution training**:\n",
    "\n",
    "- **BATCH_SIZE = 1**: Due to 4K resolution, use batch size of 1 to fit in GPU memory\n",
    "- **LEARNING_RATE = 1e-4**: Lower learning rate for stable training with large images\n",
    "- **NUM_EPOCHS = 50**: More epochs needed for complex 16× SR task\n",
    "- **TRAIN_SPLIT = 0.9**: 90% training data, 10% validation data\n",
    "\n",
    "### Data Paths\n",
    "- `HR_DIR`: Directory containing 4096×4096 high-resolution ground truth images\n",
    "- `LR_DIR`: Directory containing 256×256 low-resolution input images\n",
    "- `CHECKPOINT_DIR`: Where to save model checkpoints and training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c281987f-3b63-4bb9-9605-61cc3a31af2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数 - 针对4K训练\n",
    "BATCH_SIZE = 1  # 4K图片需要大量显存，使用batch size=1\n",
    "LEARNING_RATE = 1e-4  # 较低的学习率\n",
    "NUM_EPOCHS = 50  # 更多的训练轮数\n",
    "TRAIN_SPLIT = 0.9  # 90%训练，10%验证\n",
    "\n",
    "# 数据路径\n",
    "HR_DIR = './dataset_4k/high_resolution'\n",
    "LR_DIR = './dataset_4k/low_resolution'\n",
    "CHECKPOINT_DIR = Path('./checkpoints_4k')\n",
    "CHECKPOINT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"批次大小: {BATCH_SIZE}\")\n",
    "print(f\"学习率: {LEARNING_RATE}\")\n",
    "print(f\"训练轮数: {NUM_EPOCHS}\")\n",
    "print(\"优化策略: 深度U-Net + BatchNorm + 渐进式上采样\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l89dsmsruta",
   "metadata": {},
   "source": [
    "## 6. Data Loading and Preprocessing\n",
    "\n",
    "### Data Pipeline:\n",
    "1. **Transform**: Convert PIL images to PyTorch tensors (values in [0, 1])\n",
    "2. **Dataset**: Load full dataset with paired HR-LR images\n",
    "3. **Split**: Randomly divide into training and validation sets\n",
    "4. **DataLoader**: Create batch iterators for efficient GPU utilization\n",
    "\n",
    "**Note**: `num_workers=0` is used to avoid multiprocessing issues. `pin_memory=True` speeds up data transfer to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ca4abf-c208-4fc5-aa80-87d767f353e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据转换\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 加载数据集\n",
    "full_dataset = SRDataset4K(HR_DIR, LR_DIR, transform=transform)\n",
    "\n",
    "# 划分训练集和验证集\n",
    "train_size = int(TRAIN_SPLIT * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# 数据加载器\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                         num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                       num_workers=0, pin_memory=True)\n",
    "\n",
    "print(f\"总样本数: {len(full_dataset)}\")\n",
    "print(f\"训练集: {train_size}\")\n",
    "print(f\"验证集: {val_size}\")\n",
    "print(f\"训练批次数: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hjfa5s84pp6",
   "metadata": {},
   "source": [
    "## 7. Model Initialization\n",
    "\n",
    "### Components:\n",
    "- **Model**: UNet4K with 3 input channels (RGB) and 3 output channels (RGB)\n",
    "- **Loss Function**: L1 Loss (MAE) - better preserves sharp edges than L2 loss\n",
    "- **Optimizer**: Adam optimizer with learning rate 1e-4\n",
    "- **LR Scheduler**: StepLR - reduces learning rate by 0.5 every 15 epochs\n",
    "\n",
    "The model parameters are displayed to verify the deep architecture (~55M parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2760c112-4a5b-442f-bb90-ba36da194619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化模型\n",
    "model = UNet4K(n_channels=3, n_classes=3).to(device)\n",
    "\n",
    "# 损失函数和优化器\n",
    "criterion = nn.L1Loss()  # L1损失（MAE）\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "# 学习率调度器\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.5)\n",
    "\n",
    "# 打印模型信息\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\n模型参数总数: {total_params:,}\")\n",
    "print(f\"可训练参数: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61jri881tg8",
   "metadata": {},
   "source": [
    "## 8. Training and Validation Functions\n",
    "\n",
    "### train_epoch()\n",
    "Trains the model for one complete epoch:\n",
    "1. Set model to training mode\n",
    "2. For each batch:\n",
    "   - Forward pass: predict 4K SR images\n",
    "   - Calculate L1 loss between predictions and ground truth\n",
    "   - Backward pass: compute gradients\n",
    "   - Update model weights\n",
    "3. Return average loss for the epoch\n",
    "\n",
    "### validate()\n",
    "Evaluates model on validation set:\n",
    "1. Set model to evaluation mode (disables dropout, etc.)\n",
    "2. Disable gradient computation (`torch.no_grad()`)\n",
    "3. Calculate loss on validation batches\n",
    "4. Return average validation loss\n",
    "\n",
    "**Purpose**: Validation loss helps monitor overfitting and select the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d173755-4b74-49da-9c85-70cd3afd006b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"训练一个epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc='训练')\n",
    "    for lr_imgs, hr_imgs in pbar:\n",
    "        lr_imgs = lr_imgs.to(device)\n",
    "        hr_imgs = hr_imgs.to(device)\n",
    "        \n",
    "        # 前向传播\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(lr_imgs)\n",
    "        loss = criterion(outputs, hr_imgs)\n",
    "        \n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"验证\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for lr_imgs, hr_imgs in val_loader:\n",
    "            lr_imgs = lr_imgs.to(device)\n",
    "            hr_imgs = hr_imgs.to(device)\n",
    "            \n",
    "            outputs = model(lr_imgs)\n",
    "            loss = criterion(outputs, hr_imgs)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "    \n",
    "    return running_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rh20z6w77u",
   "metadata": {},
   "source": [
    "## 9. Training Loop\n",
    "\n",
    "This cell executes the main training loop for all epochs.\n",
    "\n",
    "### Process for Each Epoch:\n",
    "1. **Train**: Run one epoch on training set\n",
    "2. **Validate**: Evaluate on validation set\n",
    "3. **Record**: Save losses to history\n",
    "4. **Scheduler**: Update learning rate\n",
    "5. **Checkpoint**:\n",
    "   - Save best model when validation loss improves\n",
    "   - Save checkpoint every 10 epochs for backup\n",
    "\n",
    "### Monitoring:\n",
    "- Training loss should decrease steadily\n",
    "- Validation loss should track training loss (gap indicates overfitting)\n",
    "- Progress bars show real-time batch-level loss\n",
    "\n",
    "The best model is saved based on lowest validation loss, ensuring the model generalizes well to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17730366-85fa-46a4-aaf0-f390064bd714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 记录训练历史\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': []\n",
    "}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "print(\"\\n开始训练...\\n\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # 训练\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # 验证\n",
    "    val_loss = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # 记录\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    \n",
    "    # 更新学习率\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"\\n训练损失: {train_loss:.6f}\")\n",
    "    print(f\"验证损失: {val_loss:.6f}\")\n",
    "    print(f\"当前学习率: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "    \n",
    "    # 保存最佳模型\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "        }, CHECKPOINT_DIR / 'best_model.pth')\n",
    "        print(f\"✓ 保存最佳模型 (验证损失: {val_loss:.6f})\")\n",
    "    \n",
    "    # 定期保存检查点\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "        }, CHECKPOINT_DIR / f'checkpoint_epoch_{epoch+1}.pth')\n",
    "        print(f\"✓ 保存检查点: epoch_{epoch+1}\")\n",
    "\n",
    "print(\"\\n训练完成！\")\n",
    "print(f\"最佳验证损失: {best_val_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l44ef9zp8fb",
   "metadata": {},
   "source": [
    "## 10. Training Curve Visualization\n",
    "\n",
    "Plots the training and validation loss curves over all epochs.\n",
    "\n",
    "**What to Look For:**\n",
    "- **Decreasing trend**: Both losses should decrease over time\n",
    "- **Convergence**: Losses should stabilize toward the end\n",
    "- **Overfitting**: If validation loss increases while training loss decreases, the model is overfitting\n",
    "- **Underfitting**: If both losses remain high, the model needs more capacity or training\n",
    "\n",
    "The curve is saved as an image for documentation and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afde33b6-9faf-416b-91cf-06aa185a58c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制损失曲线\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history['train_loss'], label='training loss', marker='o')\n",
    "plt.plot(history['val_loss'], label='validation loss', marker='s')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (L1)')\n",
    "plt.title('4K Super-Resolution Training Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(CHECKPOINT_DIR / 'training_curve.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"训练曲线已保存: {CHECKPOINT_DIR / 'training_curve.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vunompz6dx",
   "metadata": {},
   "source": [
    "## 11. Load Best Model\n",
    "\n",
    "Load the checkpoint with the lowest validation loss for inference and evaluation.\n",
    "\n",
    "The model is set to evaluation mode to ensure consistent behavior (e.g., no dropout)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa51e86b-f6c5-4f57-8bcc-2aa3e6e9bff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载最佳模型\n",
    "checkpoint = torch.load(CHECKPOINT_DIR / 'best_model.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"已加载最佳模型 (Epoch {checkpoint['epoch']+1}, 验证损失: {checkpoint['val_loss']:.6f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pbw9ohtplhi",
   "metadata": {},
   "source": [
    "## 12. Visual Comparison of Results\n",
    "\n",
    "Generates side-by-side comparisons of:\n",
    "1. **Low Resolution Input**: The 256×256 LR image fed to the model\n",
    "2. **Model Output**: The 4K (4096×4096) super-resolved image generated by our U-Net\n",
    "3. **Ground Truth**: The actual 4096×4096 high-resolution image\n",
    "\n",
    "**Note**: Images are downsampled for display purposes.\n",
    "\n",
    "**Purpose**: Visually assess the model's performance in recovering fine details, textures, and overall image quality at 4K resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e3e3a6-ffe4-4e9d-981c-2d274bf095a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# 可视化测试结果\n",
    "num_samples = min(3, len(val_dataset))  # 4K图片较大，只显示3个样本\n",
    "fig, axes = plt.subplots(3, num_samples, figsize=(15, 15))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(num_samples):\n",
    "        # 获取一个样本\n",
    "        lr_img, hr_img = val_dataset[i]\n",
    "        lr_img_input = lr_img.unsqueeze(0).to(device)\n",
    "        \n",
    "        # 生成超分辨率图片\n",
    "        sr_img = model(lr_img_input)\n",
    "        \n",
    "        # 转换为numpy显示（下采样用于显示）\n",
    "        lr_img_np = lr_img.cpu().numpy().transpose(1, 2, 0)\n",
    "        hr_img_np = hr_img.cpu().numpy().transpose(1, 2, 0)\n",
    "        sr_img_np = sr_img.squeeze(0).cpu().numpy().transpose(1, 2, 0)\n",
    "        \n",
    "        # 裁剪到[0, 1]\n",
    "        lr_img_np = np.clip(lr_img_np, 0, 1)\n",
    "        hr_img_np = np.clip(hr_img_np, 0, 1)\n",
    "        sr_img_np = np.clip(sr_img_np, 0, 1)\n",
    "        \n",
    "        # 下采样4K图片用于显示\n",
    "        display_size = 512\n",
    "        hr_img_display = cv2.resize(hr_img_np, (display_size, display_size), interpolation=cv2.INTER_AREA)\n",
    "        sr_img_display = cv2.resize(sr_img_np, (display_size, display_size), interpolation=cv2.INTER_AREA)\n",
    "        \n",
    "        # 显示低分辨率\n",
    "        axes[0, i].imshow(lr_img_np)\n",
    "        axes[0, i].set_title('LR input (256x256)')\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # 显示生成的高分辨率\n",
    "        axes[1, i].imshow(sr_img_display)\n",
    "        axes[1, i].set_title('Model output (4096x4096)')\n",
    "        axes[1, i].axis('off')\n",
    "        \n",
    "        # 显示真实高分辨率\n",
    "        axes[2, i].imshow(hr_img_display)\n",
    "        axes[2, i].set_title('Ground truth (4096x4096)')\n",
    "        axes[2, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(CHECKPOINT_DIR / 'test_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"测试结果已保存: {CHECKPOINT_DIR / 'test_results.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ksqdqf435zf",
   "metadata": {},
   "source": [
    "## 13. Quantitative Evaluation Metrics\n",
    "\n",
    "### PSNR (Peak Signal-to-Noise Ratio)\n",
    "- Measures pixel-wise difference between images\n",
    "- **Higher is better** (typically 25-35 dB for SR tasks)\n",
    "- PSNR = 20 × log10(MAX / √MSE)\n",
    "- Sensitive to pixel-level accuracy but may not correlate perfectly with perceptual quality\n",
    "\n",
    "### SSIM (Structural Similarity Index)\n",
    "- Measures structural similarity considering luminance, contrast, and structure\n",
    "- **Range**: 0 to 1 (1 = identical images)\n",
    "- Better correlates with human perception than PSNR\n",
    "- Values above 0.95 indicate excellent quality\n",
    "\n",
    "**Note**: For 4K images, metrics are calculated on downsampled versions to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a3abdf-bba0-4c86-8478-908e97d8aea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "def calculate_metrics_4k(model, val_loader, device):\n",
    "    \"\"\"计算PSNR和SSIM (4K版本)\"\"\"\n",
    "    model.eval()\n",
    "    psnr_scores = []\n",
    "    ssim_scores = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for lr_imgs, hr_imgs in tqdm(val_loader, desc='计算评估指标'):\n",
    "            lr_imgs = lr_imgs.to(device)\n",
    "            hr_imgs = hr_imgs.to(device)\n",
    "            \n",
    "            sr_imgs = model(lr_imgs)\n",
    "            \n",
    "            # 转换为numpy\n",
    "            sr_imgs_np = sr_imgs.cpu().numpy().transpose(0, 2, 3, 1)\n",
    "            hr_imgs_np = hr_imgs.cpu().numpy().transpose(0, 2, 3, 1)\n",
    "            \n",
    "            # 计算每张图片的指标\n",
    "            for sr_img, hr_img in zip(sr_imgs_np, hr_imgs_np):\n",
    "                sr_img = np.clip(sr_img, 0, 1)\n",
    "                hr_img = np.clip(hr_img, 0, 1)\n",
    "                \n",
    "                # 为了节省内存，在下采样的图片上计算指标\n",
    "                sr_img_small = cv2.resize(sr_img, (1024, 1024), interpolation=cv2.INTER_AREA)\n",
    "                hr_img_small = cv2.resize(hr_img, (1024, 1024), interpolation=cv2.INTER_AREA)\n",
    "                \n",
    "                psnr_score = psnr(hr_img_small, sr_img_small, data_range=1.0)\n",
    "                ssim_score = ssim(hr_img_small, sr_img_small, data_range=1.0, channel_axis=2)\n",
    "                \n",
    "                psnr_scores.append(psnr_score)\n",
    "                ssim_scores.append(ssim_score)\n",
    "    \n",
    "    return np.mean(psnr_scores), np.mean(ssim_scores)\n",
    "\n",
    "# 计算指标\n",
    "avg_psnr, avg_ssim = calculate_metrics_4k(model, val_loader, device)\n",
    "\n",
    "print(\"\\n=== 评估指标 ===\")\n",
    "print(f\"平均PSNR: {avg_psnr:.2f} dB\")\n",
    "print(f\"平均SSIM: {avg_ssim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kvrzdzrmcxd",
   "metadata": {},
   "source": [
    "## 14. Training Summary\n",
    "\n",
    "Generates a comprehensive summary of the training session including:\n",
    "- Model architecture details\n",
    "- Hyperparameters used\n",
    "- Final performance metrics (loss, PSNR, SSIM)\n",
    "- File paths to saved models and results\n",
    "\n",
    "This summary is saved as a text file for future reference and experiment tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bdaa9e-8880-492d-94d6-c07bb1e51183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存训练总结\n",
    "summary = f\"\"\"\n",
    "4K超分辨率训练总结\n",
    "{'='*50}\n",
    "模型: UNet4K (深度U-Net)\n",
    "输入分辨率: 256×256\n",
    "输出分辨率: 4096×4096\n",
    "缩放倍数: 16×\n",
    "\n",
    "训练配置:\n",
    "- 训练轮数: {NUM_EPOCHS}\n",
    "- 批次大小: {BATCH_SIZE}\n",
    "- 学习率: {LEARNING_RATE}\n",
    "- 损失函数: L1 Loss\n",
    "- 优化器: Adam\n",
    "- 学习率调度: StepLR (step=15, gamma=0.5)\n",
    "\n",
    "最终结果:\n",
    "- 最佳验证损失: {best_val_loss:.6f}\n",
    "- 平均PSNR: {avg_psnr:.2f} dB\n",
    "- 平均SSIM: {avg_ssim:.4f}\n",
    "\n",
    "模型保存位置: {CHECKPOINT_DIR / 'best_model.pth'}\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "with open(CHECKPOINT_DIR / 'training_summary.txt', 'w') as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print(f\"训练总结已保存: {CHECKPOINT_DIR / 'training_summary.txt'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
