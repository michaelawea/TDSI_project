{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Patch-based Training with Memory Debugging\n",
    "\n",
    "## 特性：\n",
    "- ✅ 修复autocast兼容性问题\n",
    "- ✅ 详细的显存监控\n",
    "- ✅ 每步显示显存使用\n",
    "- ✅ 自动降低batch size如果OOM\n",
    "- ✅ 梯度累积备选方案\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utils",
   "metadata": {},
   "source": [
    "## 1. Memory Monitoring Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "memory_utils",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gpu_memory(tag=\"\"):\n",
    "    \"\"\"打印当前GPU显存使用情况\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        max_allocated = torch.cuda.max_memory_allocated() / 1024**3\n",
    "        print(f\"[{tag}] GPU显存: 已分配={allocated:.2f}GB, 保留={reserved:.2f}GB, 峰值={max_allocated:.2f}GB\")\n",
    "        return allocated, reserved, max_allocated\n",
    "    return 0, 0, 0\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"清理GPU显存\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "def get_tensor_memory(tensor):\n",
    "    \"\"\"获取tensor占用的显存(MB)\"\"\"\n",
    "    return tensor.element_size() * tensor.nelement() / 1024**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "device",
   "metadata": {},
   "source": [
    "## 2. Device Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "device_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'使用设备: {device}')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f'总显存: {total_memory:.2f} GB')\n",
    "    \n",
    "    # 重置显存统计\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    clear_gpu_memory()\n",
    "    print_gpu_memory(\"初始状态\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset",
   "metadata": {},
   "source": [
    "## 3. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataset_class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchSRDataset(Dataset):\n",
    "    \"\"\"Patch数据集\"\"\"\n",
    "    def __init__(self, hr_dir, lr_patch_size=64, scale_factor=8, \n",
    "                 patches_per_image=10, augment=True):\n",
    "        self.hr_dir = Path(hr_dir)\n",
    "        self.lr_patch_size = lr_patch_size\n",
    "        self.hr_patch_size = lr_patch_size * scale_factor\n",
    "        self.scale_factor = scale_factor\n",
    "        self.patches_per_image = patches_per_image\n",
    "        self.augment = augment\n",
    "        \n",
    "        self.hr_images = sorted(list(self.hr_dir.glob('*.png')))\n",
    "        \n",
    "        print(f\"\\n数据集配置:\")\n",
    "        print(f\"  图像数量: {len(self.hr_images)}\")\n",
    "        print(f\"  LR patch: {lr_patch_size}×{lr_patch_size}\")\n",
    "        print(f\"  HR patch: {self.hr_patch_size}×{self.hr_patch_size}\")\n",
    "        print(f\"  缩放倍数: {scale_factor}×\")\n",
    "        print(f\"  总patches: {len(self.hr_images) * patches_per_image}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.hr_images) * self.patches_per_image\n",
    "    \n",
    "    def augment_patch(self, lr, hr):\n",
    "        if random.random() > 0.5:\n",
    "            lr, hr = np.fliplr(lr), np.fliplr(hr)\n",
    "        if random.random() > 0.5:\n",
    "            lr, hr = np.flipud(lr), np.flipud(hr)\n",
    "        k = random.randint(0, 3)\n",
    "        if k > 0:\n",
    "            lr, hr = np.rot90(lr, k), np.rot90(hr, k)\n",
    "        return lr.copy(), hr.copy()\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_idx = idx // self.patches_per_image\n",
    "        \n",
    "        hr_img = cv2.imread(str(self.hr_images[img_idx]))\n",
    "        hr_img = cv2.cvtColor(hr_img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        h, w = hr_img.shape[:2]\n",
    "        max_y, max_x = h - self.hr_patch_size, w - self.hr_patch_size\n",
    "        \n",
    "        if max_y <= 0 or max_x <= 0:\n",
    "            hr_patch = cv2.resize(hr_img, (self.hr_patch_size, self.hr_patch_size))\n",
    "        else:\n",
    "            y, x = random.randint(0, max_y), random.randint(0, max_x)\n",
    "            hr_patch = hr_img[y:y+self.hr_patch_size, x:x+self.hr_patch_size]\n",
    "        \n",
    "        lr_patch = cv2.resize(hr_patch, (self.lr_patch_size, self.lr_patch_size), \n",
    "                             interpolation=cv2.INTER_CUBIC)\n",
    "        \n",
    "        if self.augment:\n",
    "            lr_patch, hr_patch = self.augment_patch(lr_patch, hr_patch)\n",
    "        \n",
    "        lr_tensor = torch.from_numpy(lr_patch.transpose(2, 0, 1)).float() / 255.0\n",
    "        hr_tensor = torch.from_numpy(hr_patch.transpose(2, 0, 1)).float() / 255.0\n",
    "        \n",
    "        return lr_tensor, hr_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model",
   "metadata": {},
   "source": [
    "## 4. Lightweight Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_def",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(ch, ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ch, ch, 3, padding=1)\n",
    "        )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.relu(x + self.conv(x))\n",
    "\n",
    "\n",
    "class TinyUNet8x(nn.Module):\n",
    "    \"\"\"超轻量8×SR模型 (64×64 → 512×512)\n",
    "    \n",
    "    设计：最小化显存占用\n",
    "    \"\"\"\n",
    "    def __init__(self, base_ch=24):  # 减少通道数到24\n",
    "        super().__init__()\n",
    "        \n",
    "        # 编码器 (64 → 32 → 16)\n",
    "        self.inc = nn.Sequential(\n",
    "            nn.Conv2d(3, base_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.down1 = nn.Sequential(\n",
    "            nn.Conv2d(base_ch, base_ch*2, 3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            ResBlock(base_ch*2)\n",
    "        )\n",
    "        \n",
    "        self.down2 = nn.Sequential(\n",
    "            nn.Conv2d(base_ch*2, base_ch*4, 3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            ResBlock(base_ch*4)\n",
    "        )\n",
    "        \n",
    "        # 上采样到8× (16 → 32 → 64 → 128 → 256 → 512)\n",
    "        self.up_blocks = nn.ModuleList([\n",
    "            self._make_up(base_ch*4, base_ch*2),  # 16→32\n",
    "            self._make_up(base_ch*2, base_ch),    # 32→64\n",
    "            self._make_up(base_ch, base_ch),      # 64→128\n",
    "            self._make_up(base_ch, base_ch),      # 128→256\n",
    "            self._make_up(base_ch, base_ch),      # 256→512\n",
    "        ])\n",
    "        \n",
    "        self.outc = nn.Conv2d(base_ch, 3, 1)\n",
    "    \n",
    "    def _make_up(self, in_ch, out_ch):\n",
    "        return nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.inc(x)\n",
    "        x = self.down1(x)\n",
    "        x = self.down2(x)\n",
    "        \n",
    "        for up in self.up_blocks:\n",
    "            x = up(x)\n",
    "        \n",
    "        return self.outc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config",
   "metadata": {},
   "source": [
    "## 5. Configuration with Auto-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config_params",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基础配置\n",
    "LR_PATCH_SIZE = 64\n",
    "SCALE_FACTOR = 8\n",
    "HR_PATCH_SIZE = LR_PATCH_SIZE * SCALE_FACTOR\n",
    "\n",
    "# 训练配置（会自动调整）\n",
    "INITIAL_BATCH_SIZE = 4  # 从小的batch size开始\n",
    "GRADIENT_ACCUMULATION = 2  # 梯度累积步数\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 50\n",
    "PATCHES_PER_IMAGE = 10\n",
    "\n",
    "BASE_CHANNELS = 24  # 减少基础通道数\n",
    "USE_MIXED_PRECISION = True\n",
    "TRAIN_SPLIT = 0.9\n",
    "\n",
    "HR_DIR = './dataset_4k/high_resolution'\n",
    "CHECKPOINT_DIR = Path('./checkpoints_debug')\n",
    "CHECKPOINT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"\\n=== 训练配置 ===\")\n",
    "print(f\"Patch: {LR_PATCH_SIZE}×{LR_PATCH_SIZE} → {HR_PATCH_SIZE}×{HR_PATCH_SIZE} ({SCALE_FACTOR}×)\")\n",
    "print(f\"初始Batch size: {INITIAL_BATCH_SIZE}\")\n",
    "print(f\"梯度累积: {GRADIENT_ACCUMULATION} steps\")\n",
    "print(f\"有效Batch size: {INITIAL_BATCH_SIZE * GRADIENT_ACCUMULATION}\")\n",
    "print(f\"基础通道数: {BASE_CHANNELS}\")\n",
    "print(f\"混合精度: {USE_MIXED_PRECISION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test_memory",
   "metadata": {},
   "source": [
    "## 6. Test Memory Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "memory_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== 显存测试 ===\")\n",
    "print(\"\\n创建模型...\")\n",
    "test_model = TinyUNet8x(base_ch=BASE_CHANNELS).to(device)\n",
    "total_params = sum(p.numel() for p in test_model.parameters())\n",
    "print(f\"模型参数: {total_params:,}\")\n",
    "print_gpu_memory(\"模型加载后\")\n",
    "\n",
    "print(\"\\n测试前向传播...\")\n",
    "test_batch_sizes = [1, 2, 4, 8]\n",
    "max_working_batch = 1\n",
    "\n",
    "for bs in test_batch_sizes:\n",
    "    try:\n",
    "        clear_gpu_memory()\n",
    "        test_lr = torch.randn(bs, 3, LR_PATCH_SIZE, LR_PATCH_SIZE).to(device)\n",
    "        test_hr = torch.randn(bs, 3, HR_PATCH_SIZE, HR_PATCH_SIZE).to(device)\n",
    "        \n",
    "        print(f\"\\n测试 batch_size={bs}:\")\n",
    "        print(f\"  输入显存: {get_tensor_memory(test_lr):.1f}MB\")\n",
    "        print(f\"  目标显存: {get_tensor_memory(test_hr):.1f}MB\")\n",
    "        \n",
    "        # 测试前向传播\n",
    "        with autocast(enabled=USE_MIXED_PRECISION):\n",
    "            out = test_model(test_lr)\n",
    "            loss = nn.L1Loss()(out, test_hr)\n",
    "        \n",
    "        alloc, _, peak = print_gpu_memory(f\"前向传播 bs={bs}\")\n",
    "        \n",
    "        # 测试反向传播\n",
    "        loss.backward()\n",
    "        alloc, _, peak = print_gpu_memory(f\"反向传播 bs={bs}\")\n",
    "        \n",
    "        max_working_batch = bs\n",
    "        print(f\"  ✓ batch_size={bs} 可用 (峰值显存: {peak:.2f}GB)\")\n",
    "        \n",
    "        del test_lr, test_hr, out, loss\n",
    "        \n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e):\n",
    "            print(f\"  ✗ batch_size={bs} OOM\")\n",
    "            break\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "clear_gpu_memory()\n",
    "del test_model\n",
    "\n",
    "print(f\"\\n推荐batch size: {max_working_batch}\")\n",
    "BATCH_SIZE = max_working_batch\n",
    "print(f\"使用batch size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_loading",
   "metadata": {},
   "source": [
    "## 7. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_loader",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PatchSRDataset(\n",
    "    hr_dir=HR_DIR,\n",
    "    lr_patch_size=LR_PATCH_SIZE,\n",
    "    scale_factor=SCALE_FACTOR,\n",
    "    patches_per_image=PATCHES_PER_IMAGE,\n",
    "    augment=True\n",
    ")\n",
    "\n",
    "train_size = int(TRAIN_SPLIT * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=0, pin_memory=True  # num_workers=0更稳定\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_set, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=0, pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\n训练集: {train_size} patches, {len(train_loader)} batches\")\n",
    "print(f\"验证集: {val_size} patches, {len(val_loader)} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_init",
   "metadata": {},
   "source": [
    "## 8. Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "init_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_gpu_memory()\n",
    "print(\"\\n初始化训练组件...\")\n",
    "\n",
    "model = TinyUNet8x(base_ch=BASE_CHANNELS).to(device)\n",
    "print_gpu_memory(\"模型\")\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
    "scaler = GradScaler() if USE_MIXED_PRECISION else None\n",
    "\n",
    "print(f\"模型参数: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train_funcs",
   "metadata": {},
   "source": [
    "## 9. Training Functions with Memory Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, scaler, device, \n",
    "                use_amp, grad_accum_steps=1, verbose_memory=False):\n",
    "    \"\"\"\n",
    "    训练一个epoch，带梯度累积和显存监控\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    pbar = tqdm(loader, desc='训练')\n",
    "    for i, (lr, hr) in enumerate(pbar):\n",
    "        lr, hr = lr.to(device), hr.to(device)\n",
    "        \n",
    "        # 前向传播\n",
    "        if use_amp and scaler:\n",
    "            with autocast(enabled=True):  # 修复：不使用device_type参数\n",
    "                out = model(lr)\n",
    "                loss = criterion(out, hr) / grad_accum_steps\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            # 梯度累积\n",
    "            if (i + 1) % grad_accum_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "        else:\n",
    "            out = model(lr)\n",
    "            loss = criterion(out, hr) / grad_accum_steps\n",
    "            loss.backward()\n",
    "            \n",
    "            if (i + 1) % grad_accum_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "        \n",
    "        running_loss += loss.item() * grad_accum_steps\n",
    "        \n",
    "        # 显示显存（每100个batch）\n",
    "        if verbose_memory and i % 100 == 0:\n",
    "            alloc, _, peak = print_gpu_memory(f\"Batch {i}\")\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item() * grad_accum_steps:.4f}',\n",
    "            'gpu': f'{torch.cuda.memory_allocated()/1024**3:.1f}GB'\n",
    "        })\n",
    "    \n",
    "    return running_loss / len(loader)\n",
    "\n",
    "\n",
    "def validate(model, loader, criterion, device, use_amp):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for lr, hr in tqdm(loader, desc='验证'):\n",
    "            lr, hr = lr.to(device), hr.to(device)\n",
    "            \n",
    "            if use_amp:\n",
    "                with autocast(enabled=True):  # 修复\n",
    "                    out = model(lr)\n",
    "                    loss = criterion(out, hr)\n",
    "            else:\n",
    "                out = model(lr)\n",
    "                loss = criterion(out, hr)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "    \n",
    "    return running_loss / len(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training",
   "metadata": {},
   "source": [
    "## 10. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_loop",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {'train_loss': [], 'val_loss': []}\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"开始训练\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # 第一个epoch显示详细显存\n",
    "    verbose = (epoch == 0)\n",
    "    \n",
    "    try:\n",
    "        train_loss = train_epoch(\n",
    "            model, train_loader, criterion, optimizer, scaler, device,\n",
    "            USE_MIXED_PRECISION, GRADIENT_ACCUMULATION, verbose_memory=verbose\n",
    "        )\n",
    "        \n",
    "        val_loss = validate(model, val_loader, criterion, device, USE_MIXED_PRECISION)\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f\"\\n训练损失: {train_loss:.6f}\")\n",
    "        print(f\"验证损失: {val_loss:.6f}\")\n",
    "        print(f\"学习率: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "        print_gpu_memory(f\"Epoch {epoch+1} 结束\")\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "                'config': {\n",
    "                    'lr_patch': LR_PATCH_SIZE,\n",
    "                    'hr_patch': HR_PATCH_SIZE,\n",
    "                    'scale': SCALE_FACTOR,\n",
    "                    'base_ch': BASE_CHANNELS\n",
    "                }\n",
    "            }, CHECKPOINT_DIR / 'best_model.pth')\n",
    "            print(f\"✓ 保存最佳模型\")\n",
    "        \n",
    "        # 定期保存\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            torch.save(model.state_dict(), CHECKPOINT_DIR / f'epoch_{epoch+1}.pth')\n",
    "            print(f\"✓ 保存检查点\")\n",
    "        \n",
    "        # 清理显存\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            clear_gpu_memory()\n",
    "    \n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e):\n",
    "            print(f\"\\n!!! OOM错误 !!!\")\n",
    "            print_gpu_memory(\"OOM时\")\n",
    "            print(\"\\n建议:\")\n",
    "            print(f\"  1. 减小batch size (当前: {BATCH_SIZE})\")\n",
    "            print(f\"  2. 增加梯度累积 (当前: {GRADIENT_ACCUMULATION})\")\n",
    "            print(f\"  3. 减小BASE_CHANNELS (当前: {BASE_CHANNELS})\")\n",
    "            print(f\"  4. 减小patch size (当前: {LR_PATCH_SIZE}→{HR_PATCH_SIZE})\")\n",
    "            raise e\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "print(f\"\\n训练完成！最佳验证损失: {best_val_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plot",
   "metadata": {},
   "source": [
    "## 11. Plot Training Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_curve",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history['train_loss'], label='Training', marker='o')\n",
    "plt.plot(history['val_loss'], label='Validation', marker='s')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'{SCALE_FACTOR}× SR Training ({LR_PATCH_SIZE}→{HR_PATCH_SIZE})')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(CHECKPOINT_DIR / 'curve.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## 12. Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, peak_memory = print_gpu_memory(\"训练结束\")\n",
    "\n",
    "summary = f\"\"\"\n",
    "训练总结\n",
    "{'='*60}\n",
    "\n",
    "配置:\n",
    "  Patch: {LR_PATCH_SIZE}×{LR_PATCH_SIZE} → {HR_PATCH_SIZE}×{HR_PATCH_SIZE} ({SCALE_FACTOR}×)\n",
    "  Batch size: {BATCH_SIZE}\n",
    "  梯度累积: {GRADIENT_ACCUMULATION}\n",
    "  有效batch: {BATCH_SIZE * GRADIENT_ACCUMULATION}\n",
    "  基础通道: {BASE_CHANNELS}\n",
    "  模型参数: {sum(p.numel() for p in model.parameters()):,}\n",
    "\n",
    "显存使用:\n",
    "  峰值显存: {peak_memory:.2f} GB\n",
    "\n",
    "结果:\n",
    "  最佳验证损失: {best_val_loss:.6f}\n",
    "  总训练轮数: {len(history['train_loss'])}\n",
    "\n",
    "模型保存: {CHECKPOINT_DIR / 'best_model.pth'}\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "with open(CHECKPOINT_DIR / 'summary.txt', 'w') as f:\n",
    "    f.write(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
