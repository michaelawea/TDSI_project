{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": "# Patch-based 4K Super-Resolution Training with Advanced Memory Debugging\n\n## 📋 概述\n\n本notebook实现了一个**基于patch的超分辨率训练系统**，专门设计用于在有限GPU显存下训练4K超分辨率模型。\n\n### 🎯 目标任务\n- **输入**: 256×256 低分辨率图像\n- **输出**: 4096×4096 高分辨率图像\n- **放大倍数**: 16× (分两阶段: 8× + 2×)\n\n### 🔑 核心特性\n\n#### 1. **Patch-based训练策略**\n- 不训练完整的大图像，而是训练小patch\n- LR patch: 64×64 → HR patch: 512×512 (8× scale)\n- 训练时显存占用极小 (~1-2GB)\n- 推理时使用滑动窗口拼接完整图像\n\n#### 2. **显存监控与调试系统** 🔍\n- 实时监控GPU显存使用情况\n- 自动测试最大可用batch size\n- 详细显示每个训练步骤的显存占用\n- OOM错误时给出具体优化建议\n\n#### 3. **自适应优化技术**\n- **混合精度训练** (FP16): 减少50%显存占用\n- **梯度累积**: 模拟更大batch size而不增加显存\n- **轻量级模型**: 最小化参数量和计算量\n- **自动batch size调整**: 根据GPU自动选择最优配置\n\n#### 4. **兼容性修复**\n- ✅ 修复PyTorch版本兼容性问题\n- ✅ 支持旧版和新版autocast API\n- ✅ 支持旧版和新版GradScaler API\n\n### 📊 显存占用估算\n\n| 组件 | 显存占用 (batch=1) | 显存占用 (batch=4) |\n|------|-------------------|-------------------|\n| 模型参数 | ~50 MB | ~50 MB |\n| 输入LR (64×64) | 0.05 MB | 0.2 MB |\n| 输出HR (512×512) | 3 MB | 12 MB |\n| 中间激活 | ~200 MB | ~800 MB |\n| 梯度 | ~50 MB | ~50 MB |\n| 优化器状态 | ~100 MB | ~100 MB |\n| **总计** | **~500 MB** | **~1.2 GB** |\n\n### 🚀 训练流程\n\n```\nStep 1: 数据准备\n  4096×4096 HR图像 → 随机裁剪512×512 patches\n  512×512 HR patch → 下采样得到64×64 LR patch\n\nStep 2: 训练8×模型\n  输入: 64×64 LR patch\n  输出: 512×512 SR patch\n  训练patches: 300张图 × 10 patches/图 = 3000 patches\n\nStep 3: 推理 (256×256 → 2048×2048)\n  256×256 → 切成4×4=16个64×64 patches\n  → 每个通过8×模型得到512×512\n  → 拼接成2048×2048\n\nStep 4: 最后放大 (2048×2048 → 4096×4096)\n  使用bicubic插值或轻量2×模型\n```\n\n### 📈 预期效果\n\n根据类似任务的经验：\n- **训练时间**: ~2-4小时 (50 epochs, RTX 3090)\n- **PSNR**: 28-32 dB\n- **SSIM**: 0.92-0.96\n- **显存占用**: <2GB\n\n### ⚙️ 配置说明\n\n可调整的关键参数：\n\n| 参数 | 默认值 | 作用 | 调整建议 |\n|------|-------|------|---------|\n| `LR_PATCH_SIZE` | 64 | LR patch大小 | 减小→降低显存 |\n| `SCALE_FACTOR` | 8 | 放大倍数 | 固定为8 |\n| `BATCH_SIZE` | 自动 | batch大小 | 自动测试 |\n| `BASE_CHANNELS` | 24 | 模型通道数 | 减小→降低显存 |\n| `GRADIENT_ACCUMULATION` | 2 | 梯度累积步数 | 增加→模拟大batch |\n\n---\n\n## 开始使用\n\n按顺序执行以下cells即可开始训练。系统会自动：\n1. 检测GPU显存\n2. 测试最优batch size\n3. 显示详细的显存使用情况\n4. 开始训练并保存最佳模型"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utils",
   "metadata": {},
   "source": "## 1. GPU显存监控工具\n\n这些工具函数用于实时监控和调试GPU显存使用情况。\n\n### 功能说明：\n\n1. **print_gpu_memory()**: 打印当前GPU显存状态\n   - `已分配`: PyTorch当前使用的显存\n   - `保留`: PyTorch从CUDA缓存池中保留的显存\n   - `峰值`: 训练过程中的最大显存占用\n\n2. **clear_gpu_memory()**: 清理GPU显存\n   - 调用Python垃圾回收\n   - 清空CUDA缓存\n   - 同步CUDA操作\n\n3. **get_tensor_memory()**: 计算单个tensor的显存占用\n   - 用于分析哪些tensor占用显存最多"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "memory_utils",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gpu_memory(tag=\"\"):\n",
    "    \"\"\"打印当前GPU显存使用情况\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        max_allocated = torch.cuda.max_memory_allocated() / 1024**3\n",
    "        print(f\"[{tag}] GPU显存: 已分配={allocated:.2f}GB, 保留={reserved:.2f}GB, 峰值={max_allocated:.2f}GB\")\n",
    "        return allocated, reserved, max_allocated\n",
    "    return 0, 0, 0\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"清理GPU显存\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "def get_tensor_memory(tensor):\n",
    "    \"\"\"获取tensor占用的显存(MB)\"\"\"\n",
    "    return tensor.element_size() * tensor.nelement() / 1024**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "device",
   "metadata": {},
   "source": "## 2. 设备配置与初始状态检查\n\n检测可用的GPU设备，并显示初始显存状态。\n\n### 输出信息：\n- GPU型号\n- 总显存容量\n- 当前显存使用情况\n\n这一步会重置显存统计，确保后续的显存测试准确。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "device_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'使用设备: {device}')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f'总显存: {total_memory:.2f} GB')\n",
    "    \n",
    "    # 重置显存统计\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    clear_gpu_memory()\n",
    "    print_gpu_memory(\"初始状态\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset",
   "metadata": {},
   "source": "## 3. Patch数据集\n\n### Patch-based训练原理\n\n传统的超分辨率训练直接使用完整的大图像，但4K图像(4096×4096)太大，无法放入GPU显存。\n\n**Patch-based方法**通过以下策略解决这个问题：\n\n1. **训练时**: 从大图中随机裁剪小patches\n   - 从4096×4096图像中裁剪512×512的HR patch\n   - 将HR patch下采样到64×64得到LR patch\n   - 每张图可以提取多个patches，增加数据多样性\n\n2. **优势**:\n   - 显存占用小（只处理512×512而非4096×4096）\n   - 数据增强丰富（每张图产生多个patches）\n   - 训练更快（小patch前向/反向传播快）\n\n3. **推理时**: 使用滑动窗口拼接\n   - 将大图切成overlapping patches\n   - 每个patch独立超分辨率\n   - 拼接成完整的大图\n\n### 数据增强\n\n为了提高模型泛化能力，对每个patch应用：\n- 随机水平翻转\n- 随机垂直翻转  \n- 随机旋转90°的倍数"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataset_class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchSRDataset(Dataset):\n",
    "    \"\"\"Patch数据集\"\"\"\n",
    "    def __init__(self, hr_dir, lr_patch_size=64, scale_factor=8, \n",
    "                 patches_per_image=10, augment=True):\n",
    "        self.hr_dir = Path(hr_dir)\n",
    "        self.lr_patch_size = lr_patch_size\n",
    "        self.hr_patch_size = lr_patch_size * scale_factor\n",
    "        self.scale_factor = scale_factor\n",
    "        self.patches_per_image = patches_per_image\n",
    "        self.augment = augment\n",
    "        \n",
    "        self.hr_images = sorted(list(self.hr_dir.glob('*.png')))\n",
    "        \n",
    "        print(f\"\\n数据集配置:\")\n",
    "        print(f\"  图像数量: {len(self.hr_images)}\")\n",
    "        print(f\"  LR patch: {lr_patch_size}×{lr_patch_size}\")\n",
    "        print(f\"  HR patch: {self.hr_patch_size}×{self.hr_patch_size}\")\n",
    "        print(f\"  缩放倍数: {scale_factor}×\")\n",
    "        print(f\"  总patches: {len(self.hr_images) * patches_per_image}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.hr_images) * self.patches_per_image\n",
    "    \n",
    "    def augment_patch(self, lr, hr):\n",
    "        if random.random() > 0.5:\n",
    "            lr, hr = np.fliplr(lr), np.fliplr(hr)\n",
    "        if random.random() > 0.5:\n",
    "            lr, hr = np.flipud(lr), np.flipud(hr)\n",
    "        k = random.randint(0, 3)\n",
    "        if k > 0:\n",
    "            lr, hr = np.rot90(lr, k), np.rot90(hr, k)\n",
    "        return lr.copy(), hr.copy()\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_idx = idx // self.patches_per_image\n",
    "        \n",
    "        hr_img = cv2.imread(str(self.hr_images[img_idx]))\n",
    "        hr_img = cv2.cvtColor(hr_img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        h, w = hr_img.shape[:2]\n",
    "        max_y, max_x = h - self.hr_patch_size, w - self.hr_patch_size\n",
    "        \n",
    "        if max_y <= 0 or max_x <= 0:\n",
    "            hr_patch = cv2.resize(hr_img, (self.hr_patch_size, self.hr_patch_size))\n",
    "        else:\n",
    "            y, x = random.randint(0, max_y), random.randint(0, max_x)\n",
    "            hr_patch = hr_img[y:y+self.hr_patch_size, x:x+self.hr_patch_size]\n",
    "        \n",
    "        lr_patch = cv2.resize(hr_patch, (self.lr_patch_size, self.lr_patch_size), \n",
    "                             interpolation=cv2.INTER_CUBIC)\n",
    "        \n",
    "        if self.augment:\n",
    "            lr_patch, hr_patch = self.augment_patch(lr_patch, hr_patch)\n",
    "        \n",
    "        lr_tensor = torch.from_numpy(lr_patch.transpose(2, 0, 1)).float() / 255.0\n",
    "        hr_tensor = torch.from_numpy(hr_patch.transpose(2, 0, 1)).float() / 255.0\n",
    "        \n",
    "        return lr_tensor, hr_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model",
   "metadata": {},
   "source": "## 4. 轻量级U-Net模型架构\n\n### 模型设计原则\n\n为了在有限显存下训练，我们采用以下设计：\n\n#### 1. **减少通道数** \n- 基础通道数设为24（而非常见的64）\n- 最深层通道数为96（而非512）\n- 参数量减少约75%\n\n#### 2. **移除BatchNorm**\n- BatchNorm需要额外显存存储running stats\n- 对于小batch size，BN效果不佳\n- 使用残差连接保证训练稳定性\n\n#### 3. **浅层编码器**\n- 只下采样2次（64→32→16）\n- 避免过小的feature map\n- 保留更多空间信息\n\n#### 4. **渐进式上采样**\n- 从16×16逐步上采样到512×512\n- 使用最近邻插值+卷积（比转置卷积省显存）\n- 5次2×上采样达到32×放大（16→512）\n\n### 架构流程\n\n```\n输入: 64×64×3\n\n编码器:\n  64×64×3 → Conv → 64×64×24\n  64×64×24 → DownConv → 32×32×48\n  32×32×48 → DownConv → 16×16×96\n\n解码器（8×上采样）:\n  16×16×96 → Up → 32×32×48\n  32×32×48 → Up → 64×64×24\n  64×64×24 → Up → 128×128×24\n  128×128×24 → Up → 256×256×24\n  256×256×24 → Up → 512×512×24\n  512×512×24 → Conv1×1 → 512×512×3\n\n输出: 512×512×3\n```\n\n### 参数量估算\n\n- 编码器: ~50K parameters\n- 解码器: ~200K parameters\n- **总计: ~250K parameters** (相比原版UNet减少95%)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_def",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(ch, ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ch, ch, 3, padding=1)\n",
    "        )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.relu(x + self.conv(x))\n",
    "\n",
    "\n",
    "class TinyUNet8x(nn.Module):\n",
    "    \"\"\"超轻量8×SR模型 (64×64 → 512×512)\n",
    "    \n",
    "    设计：最小化显存占用\n",
    "    \"\"\"\n",
    "    def __init__(self, base_ch=24):  # 减少通道数到24\n",
    "        super().__init__()\n",
    "        \n",
    "        # 编码器 (64 → 32 → 16)\n",
    "        self.inc = nn.Sequential(\n",
    "            nn.Conv2d(3, base_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.down1 = nn.Sequential(\n",
    "            nn.Conv2d(base_ch, base_ch*2, 3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            ResBlock(base_ch*2)\n",
    "        )\n",
    "        \n",
    "        self.down2 = nn.Sequential(\n",
    "            nn.Conv2d(base_ch*2, base_ch*4, 3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            ResBlock(base_ch*4)\n",
    "        )\n",
    "        \n",
    "        # 上采样到8× (16 → 32 → 64 → 128 → 256 → 512)\n",
    "        self.up_blocks = nn.ModuleList([\n",
    "            self._make_up(base_ch*4, base_ch*2),  # 16→32\n",
    "            self._make_up(base_ch*2, base_ch),    # 32→64\n",
    "            self._make_up(base_ch, base_ch),      # 64→128\n",
    "            self._make_up(base_ch, base_ch),      # 128→256\n",
    "            self._make_up(base_ch, base_ch),      # 256→512\n",
    "        ])\n",
    "        \n",
    "        self.outc = nn.Conv2d(base_ch, 3, 1)\n",
    "    \n",
    "    def _make_up(self, in_ch, out_ch):\n",
    "        return nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.inc(x)\n",
    "        x = self.down1(x)\n",
    "        x = self.down2(x)\n",
    "        \n",
    "        for up in self.up_blocks:\n",
    "            x = up(x)\n",
    "        \n",
    "        return self.outc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config",
   "metadata": {},
   "source": "## 5. 训练配置与自适应优化\n\n### 核心配置参数\n\n#### Patch大小\n- **LR_PATCH_SIZE**: 64×64 - 低分辨率patch大小\n- **HR_PATCH_SIZE**: 512×512 - 高分辨率patch大小  \n- **SCALE_FACTOR**: 8× - 本模型的放大倍数\n\n#### 训练超参数\n- **BATCH_SIZE**: 自动检测 - 根据GPU显存自动选择\n- **GRADIENT_ACCUMULATION**: 2 - 梯度累积步数\n  - 有效batch = BATCH_SIZE × GRADIENT_ACCUMULATION\n  - 例如: 2 × 2 = 4 (模拟batch=4的效果)\n- **LEARNING_RATE**: 1e-4 - 学习率\n- **NUM_EPOCHS**: 50 - 训练轮数\n\n#### 模型配置\n- **BASE_CHANNELS**: 24 - 基础通道数（越小显存越少）\n- **PATCHES_PER_IMAGE**: 10 - 每张图提取的patch数量\n\n### 优化技术详解\n\n#### 1. 梯度累积 (Gradient Accumulation)\n\n**问题**: GPU显存有限，batch size只能设为1或2，训练不稳定\n\n**解决**: 累积多个mini-batch的梯度再更新\n\n```python\nfor i, (input, target) in enumerate(loader):\n    loss = model(input, target) / accumulation_steps\n    loss.backward()  # 累积梯度\n    \n    if (i + 1) % accumulation_steps == 0:\n        optimizer.step()  # 更新权重\n        optimizer.zero_grad()  # 清空梯度\n```\n\n**效果**: batch=2, accumulation=2 ≈ batch=4的训练效果\n\n#### 2. 混合精度训练 (Mixed Precision)\n\n**原理**: \n- FP32: 32位浮点数（高精度，高显存）\n- FP16: 16位浮点数（低精度，低显存）\n- 混合精度: 大部分操作用FP16，关键操作用FP32\n\n**显存节省**: 约50%\n\n**精度损失**: 几乎没有（<0.1%）"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config_params",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基础配置\n",
    "LR_PATCH_SIZE = 64\n",
    "SCALE_FACTOR = 8\n",
    "HR_PATCH_SIZE = LR_PATCH_SIZE * SCALE_FACTOR\n",
    "\n",
    "# 训练配置（会自动调整）\n",
    "INITIAL_BATCH_SIZE = 4  # 从小的batch size开始\n",
    "GRADIENT_ACCUMULATION = 2  # 梯度累积步数\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 50\n",
    "PATCHES_PER_IMAGE = 10\n",
    "\n",
    "BASE_CHANNELS = 24  # 减少基础通道数\n",
    "USE_MIXED_PRECISION = True\n",
    "TRAIN_SPLIT = 0.9\n",
    "\n",
    "HR_DIR = './dataset_4k/high_resolution'\n",
    "CHECKPOINT_DIR = Path('./checkpoints_debug')\n",
    "CHECKPOINT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"\\n=== 训练配置 ===\")\n",
    "print(f\"Patch: {LR_PATCH_SIZE}×{LR_PATCH_SIZE} → {HR_PATCH_SIZE}×{HR_PATCH_SIZE} ({SCALE_FACTOR}×)\")\n",
    "print(f\"初始Batch size: {INITIAL_BATCH_SIZE}\")\n",
    "print(f\"梯度累积: {GRADIENT_ACCUMULATION} steps\")\n",
    "print(f\"有效Batch size: {INITIAL_BATCH_SIZE * GRADIENT_ACCUMULATION}\")\n",
    "print(f\"基础通道数: {BASE_CHANNELS}\")\n",
    "print(f\"混合精度: {USE_MIXED_PRECISION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test_memory",
   "metadata": {},
   "source": "## 6. 显存测试与Batch Size自动检测\n\n### 测试目的\n\n在开始训练前，自动测试不同batch size下的显存占用，找出最大可用batch size。\n\n### 测试流程\n\n```\nFor batch_size in [1, 2, 4, 8]:\n    1. 创建随机输入数据\n    2. 前向传播 → 记录显存\n    3. 计算损失\n    4. 反向传播 → 记录显存\n    5. 如果OOM，停止测试\n```\n\n### 输出信息\n\n对于每个batch size，显示：\n- 输入tensor显存占用\n- 输出tensor显存占用  \n- 前向传播后的总显存\n- 反向传播后的峰值显存\n- 是否可用\n\n### 自动选择策略\n\n- 选择能成功运行的最大batch size\n- 如果最大batch size较小（1-2），建议使用梯度累积"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "memory_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== 显存测试 ===\")\n",
    "print(\"\\n创建模型...\")\n",
    "test_model = TinyUNet8x(base_ch=BASE_CHANNELS).to(device)\n",
    "total_params = sum(p.numel() for p in test_model.parameters())\n",
    "print(f\"模型参数: {total_params:,}\")\n",
    "print_gpu_memory(\"模型加载后\")\n",
    "\n",
    "print(\"\\n测试前向传播...\")\n",
    "test_batch_sizes = [1, 2, 4, 8]\n",
    "max_working_batch = 1\n",
    "\n",
    "for bs in test_batch_sizes:\n",
    "    try:\n",
    "        clear_gpu_memory()\n",
    "        test_lr = torch.randn(bs, 3, LR_PATCH_SIZE, LR_PATCH_SIZE).to(device)\n",
    "        test_hr = torch.randn(bs, 3, HR_PATCH_SIZE, HR_PATCH_SIZE).to(device)\n",
    "        \n",
    "        print(f\"\\n测试 batch_size={bs}:\")\n",
    "        print(f\"  输入显存: {get_tensor_memory(test_lr):.1f}MB\")\n",
    "        print(f\"  目标显存: {get_tensor_memory(test_hr):.1f}MB\")\n",
    "        \n",
    "        # 测试前向传播\n",
    "        with autocast(enabled=USE_MIXED_PRECISION):\n",
    "            out = test_model(test_lr)\n",
    "            loss = nn.L1Loss()(out, test_hr)\n",
    "        \n",
    "        alloc, _, peak = print_gpu_memory(f\"前向传播 bs={bs}\")\n",
    "        \n",
    "        # 测试反向传播\n",
    "        loss.backward()\n",
    "        alloc, _, peak = print_gpu_memory(f\"反向传播 bs={bs}\")\n",
    "        \n",
    "        max_working_batch = bs\n",
    "        print(f\"  ✓ batch_size={bs} 可用 (峰值显存: {peak:.2f}GB)\")\n",
    "        \n",
    "        del test_lr, test_hr, out, loss\n",
    "        \n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e):\n",
    "            print(f\"  ✗ batch_size={bs} OOM\")\n",
    "            break\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "clear_gpu_memory()\n",
    "del test_model\n",
    "\n",
    "print(f\"\\n推荐batch size: {max_working_batch}\")\n",
    "BATCH_SIZE = max_working_batch\n",
    "print(f\"使用batch size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_loading",
   "metadata": {},
   "source": "## 7. 数据加载与训练/验证集划分\n\n### 数据加载配置\n\n使用PyTorch的DataLoader加载patch数据集，关键配置：\n\n#### DataLoader参数\n- **batch_size**: 使用前面自动检测的最大batch size\n- **shuffle**: 训练集shuffle=True，打乱顺序增强泛化\n- **num_workers**: 设为0（单进程加载）\n  - 多进程加载可能增加显存开销\n  - 对于小patch，单进程已足够快\n- **pin_memory**: True，加速GPU数据传输\n\n#### 训练/验证集划分\n- **训练集**: 90%的patches (用于训练模型)\n- **验证集**: 10%的patches (用于监控过拟合)\n\n### 数据量计算\n\n假设有300张4K图像，每张提取10个patches：\n- 总patches: 300 × 10 = 3000\n- 训练patches: 3000 × 0.9 = 2700\n- 验证patches: 3000 × 0.1 = 300\n\n如果batch_size=2：\n- 训练batches: 2700 / 2 = 1350\n- 验证batches: 300 / 2 = 150"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_loader",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PatchSRDataset(\n",
    "    hr_dir=HR_DIR,\n",
    "    lr_patch_size=LR_PATCH_SIZE,\n",
    "    scale_factor=SCALE_FACTOR,\n",
    "    patches_per_image=PATCHES_PER_IMAGE,\n",
    "    augment=True\n",
    ")\n",
    "\n",
    "train_size = int(TRAIN_SPLIT * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=0, pin_memory=True  # num_workers=0更稳定\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_set, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=0, pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\n训练集: {train_size} patches, {len(train_loader)} batches\")\n",
    "print(f\"验证集: {val_size} patches, {len(val_loader)} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_init",
   "metadata": {},
   "source": "## 8. 模型与优化器初始化\n\n### 训练组件\n\n#### 1. 模型 (TinyUNet8x)\n- 加载到GPU设备\n- 基础通道数: 24\n- 总参数量: ~250K\n\n#### 2. 损失函数 (L1Loss)\n- 也称为MAE (Mean Absolute Error)\n- 比MSE更关注大误差，适合图像超分辨率\n- 公式: L = |预测 - 真实| 的平均值\n\n#### 3. 优化器 (Adam)\n- 学习率: 1e-4 (0.0001)\n- Adam自适应调整每个参数的学习率\n- 适合处理稀疏梯度和噪声数据\n\n#### 4. 学习率调度器 (CosineAnnealingLR)\n- 余弦退火策略\n- 学习率从初始值逐渐降低到接近0\n- 前期学习快，后期微调\n- 公式: lr = lr_min + (lr_max - lr_min) × (1 + cos(π × epoch / T_max)) / 2\n\n#### 5. 梯度缩放器 (GradScaler)\n- 仅在混合精度训练时使用\n- 防止FP16下梯度下溢\n- 自动缩放loss以保持梯度数值稳定性\n\n### 初始化后显存\n\n模型参数占用约50MB显存。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "init_model",
   "metadata": {},
   "outputs": [],
   "source": "clear_gpu_memory()\nprint(\"\\n初始化训练组件...\")\n\nmodel = TinyUNet8x(base_ch=BASE_CHANNELS).to(device)\nprint_gpu_memory(\"模型\")\n\ncriterion = nn.L1Loss()\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n\n# 兼容新旧PyTorch版本的GradScaler\nif USE_MIXED_PRECISION:\n    try:\n        # 新版API (PyTorch >= 2.0)\n        scaler = torch.amp.GradScaler('cuda')\n    except AttributeError:\n        # 旧版API (PyTorch < 2.0)\n        scaler = GradScaler()\nelse:\n    scaler = None\n\nprint(f\"模型参数: {sum(p.numel() for p in model.parameters()):,}\")"
  },
  {
   "cell_type": "markdown",
   "id": "train_funcs",
   "metadata": {},
   "source": "## 9. 训练与验证函数\n\n### train_epoch() - 训练一个epoch\n\n#### 参数说明\n- **grad_accum_steps**: 梯度累积步数\n- **verbose_memory**: 是否详细打印显存使用\n\n#### 梯度累积实现\n\n```python\noptimizer.zero_grad()  # 初始化梯度为0\n\nfor i, (lr, hr) in enumerate(loader):\n    loss = criterion(out, hr) / grad_accum_steps  # 除以累积步数\n    loss.backward()  # 累积梯度（不立即更新权重）\n\n    # 每grad_accum_steps个batch才更新一次\n    if (i + 1) % grad_accum_steps == 0:\n        optimizer.step()  # 应用累积的梯度\n        optimizer.zero_grad()  # 清空梯度\n```\n\n**效果**:\n- batch_size=2, grad_accum=2 → 等效于batch_size=4\n- 显存占用仍然只是batch_size=2的量\n\n#### 混合精度训练流程\n\n```python\nwith autocast(enabled=True):  # 自动转换为FP16\n    out = model(lr)\n    loss = criterion(out, hr)\n\nscaler.scale(loss).backward()  # 缩放loss防止梯度下溢\nscaler.step(optimizer)  # 应用缩放后的梯度\nscaler.update()  # 更新scaler的缩放因子\n```\n\n#### 进度显示\n- tqdm进度条显示训练进度\n- 实时显示当前loss和GPU显存占用\n- 第一个epoch显示详细显存信息（每100个batch）\n\n### validate() - 验证函数\n\n- 使用torch.no_grad()禁用梯度计算\n- 节省显存和计算时间\n- 返回验证集平均loss"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, scaler, device, \n",
    "                use_amp, grad_accum_steps=1, verbose_memory=False):\n",
    "    \"\"\"\n",
    "    训练一个epoch，带梯度累积和显存监控\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    pbar = tqdm(loader, desc='训练')\n",
    "    for i, (lr, hr) in enumerate(pbar):\n",
    "        lr, hr = lr.to(device), hr.to(device)\n",
    "        \n",
    "        # 前向传播\n",
    "        if use_amp and scaler:\n",
    "            with autocast(enabled=True):  # 修复：不使用device_type参数\n",
    "                out = model(lr)\n",
    "                loss = criterion(out, hr) / grad_accum_steps\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            # 梯度累积\n",
    "            if (i + 1) % grad_accum_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "        else:\n",
    "            out = model(lr)\n",
    "            loss = criterion(out, hr) / grad_accum_steps\n",
    "            loss.backward()\n",
    "            \n",
    "            if (i + 1) % grad_accum_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "        \n",
    "        running_loss += loss.item() * grad_accum_steps\n",
    "        \n",
    "        # 显示显存（每100个batch）\n",
    "        if verbose_memory and i % 100 == 0:\n",
    "            alloc, _, peak = print_gpu_memory(f\"Batch {i}\")\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item() * grad_accum_steps:.4f}',\n",
    "            'gpu': f'{torch.cuda.memory_allocated()/1024**3:.1f}GB'\n",
    "        })\n",
    "    \n",
    "    return running_loss / len(loader)\n",
    "\n",
    "\n",
    "def validate(model, loader, criterion, device, use_amp):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for lr, hr in tqdm(loader, desc='验证'):\n",
    "            lr, hr = lr.to(device), hr.to(device)\n",
    "            \n",
    "            if use_amp:\n",
    "                with autocast(enabled=True):  # 修复\n",
    "                    out = model(lr)\n",
    "                    loss = criterion(out, hr)\n",
    "            else:\n",
    "                out = model(lr)\n",
    "                loss = criterion(out, hr)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "    \n",
    "    return running_loss / len(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training",
   "metadata": {},
   "source": "## 10. 主训练循环\n\n### 训练流程\n\n对于每个epoch：\n\n1. **训练阶段**\n   - 调用train_epoch()在训练集上训练\n   - 第一个epoch显示详细显存信息\n   - 返回平均训练损失\n\n2. **验证阶段**\n   - 调用validate()在验证集上评估\n   - 返回平均验证损失\n\n3. **学习率调整**\n   - scheduler.step()应用余弦退火\n\n4. **模型保存策略**\n   - **最佳模型**: 验证损失最低时保存为`best_model.pth`\n     - 包含模型权重、优化器状态、配置信息\n   - **定期检查点**: 每10个epoch保存为`epoch_N.pth`\n     - 便于恢复训练或分析不同阶段的模型\n\n5. **显存管理**\n   - 每5个epoch清理一次GPU缓存\n   - 每个epoch结束打印显存统计\n\n### OOM错误处理\n\n如果训练中出现OOM错误，自动显示：\n- 当前显存使用情况\n- 优化建议：\n  1. 减小batch size\n  2. 增加梯度累积步数\n  3. 减小模型通道数\n  4. 减小patch size\n\n### 输出信息\n\n每个epoch显示：\n- 训练损失和验证损失\n- 当前学习率\n- 显存使用情况\n- 模型保存状态"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_loop",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {'train_loss': [], 'val_loss': []}\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"开始训练\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # 第一个epoch显示详细显存\n",
    "    verbose = (epoch == 0)\n",
    "    \n",
    "    try:\n",
    "        train_loss = train_epoch(\n",
    "            model, train_loader, criterion, optimizer, scaler, device,\n",
    "            USE_MIXED_PRECISION, GRADIENT_ACCUMULATION, verbose_memory=verbose\n",
    "        )\n",
    "        \n",
    "        val_loss = validate(model, val_loader, criterion, device, USE_MIXED_PRECISION)\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f\"\\n训练损失: {train_loss:.6f}\")\n",
    "        print(f\"验证损失: {val_loss:.6f}\")\n",
    "        print(f\"学习率: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "        print_gpu_memory(f\"Epoch {epoch+1} 结束\")\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "                'config': {\n",
    "                    'lr_patch': LR_PATCH_SIZE,\n",
    "                    'hr_patch': HR_PATCH_SIZE,\n",
    "                    'scale': SCALE_FACTOR,\n",
    "                    'base_ch': BASE_CHANNELS\n",
    "                }\n",
    "            }, CHECKPOINT_DIR / 'best_model.pth')\n",
    "            print(f\"✓ 保存最佳模型\")\n",
    "        \n",
    "        # 定期保存\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            torch.save(model.state_dict(), CHECKPOINT_DIR / f'epoch_{epoch+1}.pth')\n",
    "            print(f\"✓ 保存检查点\")\n",
    "        \n",
    "        # 清理显存\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            clear_gpu_memory()\n",
    "    \n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e):\n",
    "            print(f\"\\n!!! OOM错误 !!!\")\n",
    "            print_gpu_memory(\"OOM时\")\n",
    "            print(\"\\n建议:\")\n",
    "            print(f\"  1. 减小batch size (当前: {BATCH_SIZE})\")\n",
    "            print(f\"  2. 增加梯度累积 (当前: {GRADIENT_ACCUMULATION})\")\n",
    "            print(f\"  3. 减小BASE_CHANNELS (当前: {BASE_CHANNELS})\")\n",
    "            print(f\"  4. 减小patch size (当前: {LR_PATCH_SIZE}→{HR_PATCH_SIZE})\")\n",
    "            raise e\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "print(f\"\\n训练完成！最佳验证损失: {best_val_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plot",
   "metadata": {},
   "source": "## 11. 训练曲线可视化\n\n### 损失曲线图\n\n绘制训练损失和验证损失随epoch的变化：\n\n- **蓝色线 (Training)**: 训练集损失\n  - 应该持续下降\n  - 如果不下降，学习率可能太小或模型容量不足\n\n- **橙色线 (Validation)**: 验证集损失\n  - 用于判断是否过拟合\n  - 如果验证损失上升而训练损失下降 → 过拟合\n\n### 理想曲线特征\n\n✅ **健康的训练**:\n- 训练和验证损失都持续下降\n- 验证损失略高于训练损失\n- 两条曲线走势相似\n\n⚠️ **过拟合警告**:\n- 训练损失很低，验证损失很高\n- 验证损失开始上升\n\n⚠️ **欠拟合警告**:\n- 两个损失都很高且不下降\n- 需要增加模型容量或训练更长时间\n\n### 保存\n\n曲线图自动保存到`checkpoints_debug/curve.png`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_curve",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history['train_loss'], label='Training', marker='o')\n",
    "plt.plot(history['val_loss'], label='Validation', marker='s')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'{SCALE_FACTOR}× SR Training ({LR_PATCH_SIZE}→{HR_PATCH_SIZE})')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(CHECKPOINT_DIR / 'curve.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": "## 12. 训练总结报告\n\n### 总结内容\n\n生成完整的训练总结，包括：\n\n#### 配置信息\n- Patch大小和缩放倍数\n- Batch size和梯度累积配置\n- 模型通道数和参数量\n\n#### 显存统计\n- 训练过程中的峰值显存占用\n- 用于评估是否可以进一步增加batch size或模型大小\n\n#### 训练结果\n- 最佳验证损失\n- 总训练轮数\n- 模型保存路径\n\n### 文件输出\n\n总结报告会：\n1. 打印到控制台\n2. 保存为文本文件: `checkpoints_debug/summary.txt`\n\n### 后续步骤\n\n训练完成后：\n\n1. **加载最佳模型**\n```python\ncheckpoint = torch.load('checkpoints_debug/best_model.pth')\nmodel.load_state_dict(checkpoint['model_state_dict'])\n```\n\n2. **推理测试**\n- 使用滑动窗口将256×256图像超分辨率到2048×2048\n- 再用bicubic插值放大到4096×4096\n\n3. **质量评估**\n- 计算PSNR, SSIM指标\n- 与bicubic/其他方法对比"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, peak_memory = print_gpu_memory(\"训练结束\")\n",
    "\n",
    "summary = f\"\"\"\n",
    "训练总结\n",
    "{'='*60}\n",
    "\n",
    "配置:\n",
    "  Patch: {LR_PATCH_SIZE}×{LR_PATCH_SIZE} → {HR_PATCH_SIZE}×{HR_PATCH_SIZE} ({SCALE_FACTOR}×)\n",
    "  Batch size: {BATCH_SIZE}\n",
    "  梯度累积: {GRADIENT_ACCUMULATION}\n",
    "  有效batch: {BATCH_SIZE * GRADIENT_ACCUMULATION}\n",
    "  基础通道: {BASE_CHANNELS}\n",
    "  模型参数: {sum(p.numel() for p in model.parameters()):,}\n",
    "\n",
    "显存使用:\n",
    "  峰值显存: {peak_memory:.2f} GB\n",
    "\n",
    "结果:\n",
    "  最佳验证损失: {best_val_loss:.6f}\n",
    "  总训练轮数: {len(history['train_loss'])}\n",
    "\n",
    "模型保存: {CHECKPOINT_DIR / 'best_model.pth'}\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "with open(CHECKPOINT_DIR / 'summary.txt', 'w') as f:\n",
    "    f.write(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}